{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee55cfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_AGENT = 'Channel1Scraper:1.0 (by u/AlarmingSignal4018)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6511271e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¥ Top 20 rising posts in r/all ðŸ”¥\n",
      "Maine/everyone deserves better than Susan Collins | ðŸ‘ 839 | ðŸ’¬ 13\n",
      "Democratic candidate running for U.S. Senate in Maine, Graham Platner: \"No one cares that you pretend to be remorseful as you sell out to lobbyists. Symbolic opposition does not reopen hospitals. Weak condemnations do not bring back Roe V Wade. Maine deserves better than Susan Collins\" | ðŸ‘ 870 | ðŸ’¬ 15\n",
      "Aaaany second now... | ðŸ‘ 349 | ðŸ’¬ 22\n",
      "So very Trumpian | ðŸ‘ 691 | ðŸ’¬ 25\n",
      "The safety of a rally car | ðŸ‘ 1869 | ðŸ’¬ 98\n",
      "Zohran Mamdani responds to Islamophobic trolling in the best way | ðŸ‘ 1859 | ðŸ’¬ 67\n",
      "Aquatic plant producing oxygen | ðŸ‘ 1007 | ðŸ’¬ 28\n",
      "Adulthood in One Sentence | ðŸ‘ 187 | ðŸ’¬ 3\n",
      "My mom found somebody's tooth implant in her quesadilla | ðŸ‘ 253 | ðŸ’¬ 53\n",
      "Meow~ (@JizokuArt) | ðŸ‘ 438 | ðŸ’¬ 35\n",
      "Democratic Rep. Jerry Nadler to retire from Congress | ðŸ‘ 310 | ðŸ’¬ 36\n",
      "Stop falling for \"luxury\" apartments. | ðŸ‘ 150 | ðŸ’¬ 44\n",
      "Bike theft gone wrong | ðŸ‘ 230 | ðŸ’¬ 39\n",
      "Dafne Keen clink | ðŸ‘ 128 | ðŸ’¬ 1\n",
      "Future nominee Dwayne Johnson (The Smashing Machine) gets emotional during 15-minute standing ovation at Venice | ðŸ‘ 104 | ðŸ’¬ 57\n",
      "Maine/everyone deserves better than Susan Collins | ðŸ‘ 611 | ðŸ’¬ 7\n",
      "More 4 primarchâ€™s drawn by me | ðŸ‘ 132 | ðŸ’¬ 8\n",
      "blursed woman | ðŸ‘ 508 | ðŸ’¬ 67\n",
      "Coaxed into mascot horrors with like a trillion entries | ðŸ‘ 331 | ðŸ’¬ 30\n",
      "Raw milk people are dumb. Sorry not sorry. | ðŸ‘ 577 | ðŸ’¬ 90\n",
      "\n",
      "âœ… Saved trending stories to trending.json\n"
     ]
    }
   ],
   "source": [
    "# pull_stories.py\n",
    "\n",
    "import praw\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "\n",
    "load_dotenv(find_dotenv(usecwd=True), override=True)\n",
    "\n",
    "client_secret = os.getenv(\"CLIENT_SECRET\")\n",
    "user_id = os.getenv(\"USER_ID\")\n",
    "\n",
    "# Connect to Reddit\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.getenv(\"CLIENT_ID\"),\n",
    "    client_secret=os.getenv(\"CLIENT_SECRET\"),\n",
    "    user_agent= USER_AGENT\n",
    ")\n",
    "\n",
    "def fetch_trending(subreddit_name=\"all\", limit=10):\n",
    "    \"\"\"Fetch trending stories from a subreddit (default: all)\"\"\"\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    stories = []\n",
    "\n",
    "    print(f\"\\nðŸ”¥ Top {limit} rising posts in r/{subreddit_name} ðŸ”¥\")\n",
    "    for post in subreddit.rising(limit=limit):\n",
    "        story = {\n",
    "            \"title\": post.title,\n",
    "            \"score\": post.score,\n",
    "            \"comments\": post.num_comments,\n",
    "            \"url\": post.url,\n",
    "            \"text\": post.selftext[:500]  # preview first 500 chars\n",
    "        }\n",
    "        stories.append(story)\n",
    "        print(f\"{story['title']} | ðŸ‘ {story['score']} | ðŸ’¬ {story['comments']}\")\n",
    "\n",
    "    return stories\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trending = fetch_trending(limit=20)\n",
    "\n",
    "    # Save results to JSON\n",
    "    import json\n",
    "    with open(\"trending.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(trending, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(\"\\nâœ… Saved trending stories to trending.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dbfc075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“š Long high-engagement posts from r/AskReddit:\n",
      "\n",
      "Title: Trump said today: â€œI have the right to do anything I want to do. Iâ€™m the president of the United Statesâ€. People from the US, what are your thoughts on this?\n",
      "ðŸ‘ 57263 | ðŸ’¬ 13705\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n0xxaf/trump_said_today_i_have_the_right_to_do_anything/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What is a job that pays extremely well but no one realizes it?\n",
      "ðŸ‘ 13166 | ðŸ’¬ 6077\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n4g11a/what_is_a_job_that_pays_extremely_well_but_no_one/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: To those who have left the MAGA movement, what was your wake-up call?\n",
      "ðŸ‘ 11190 | ðŸ’¬ 6863\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n14pol/to_those_who_have_left_the_maga_movement_what_was/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What would the GOP power struggle look like if Donald Trump dies?\n",
      "ðŸ‘ 8667 | ðŸ’¬ 2955\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n40xp2/what_would_the_gop_power_struggle_look_like_if/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What Do You Think Would Be The Condition Of The United States Today if Hillary Clinton Had Become President in January 2017?\n",
      "ðŸ‘ 8336 | ðŸ’¬ 5032\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n4ulw7/what_do_you_think_would_be_the_condition_of_the/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: Whatâ€™s the first sign you usually notice when your mental health starts declining?\n",
      "ðŸ‘ 8420 | ðŸ’¬ 2983\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n5aukv/whats_the_first_sign_you_usually_notice_when_your/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What is a little quirk about your body that you donâ€™t think other people have?\n",
      "ðŸ‘ 7343 | ðŸ’¬ 12435\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n3qqcs/what_is_a_little_quirk_about_your_body_that_you/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: How would you feel about your partner waking you up just to have sex?\n",
      "ðŸ‘ 6677 | ðŸ’¬ 1811\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n32iaw/how_would_you_feel_about_your_partner_waking_you/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: Men: What's the brutally honest dating advice you wish the women in your life (sisters, daughters, friends, etc.) understood?\n",
      "ðŸ‘ 6463 | ðŸ’¬ 3328\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n3liug/men_whats_the_brutally_honest_dating_advice_you/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What is the best yet smallest sign of seduction?\n",
      "ðŸ‘ 5937 | ðŸ’¬ 649\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n58cvt/what_is_the_best_yet_smallest_sign_of_seduction/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: To all the married women, what is that something your husband does that makes think you won the lottery in marriage aspect of life?\n",
      "ðŸ‘ 5870 | ðŸ’¬ 1244\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n3cp8q/to_all_the_married_women_what_is_that_something/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What screams \"I have a crush on you\"?\n",
      "ðŸ‘ 5719 | ðŸ’¬ 1000\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n0yhni/what_screams_i_have_a_crush_on_you/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: People who used the internet before 2001, what did you do in there?\n",
      "ðŸ‘ 5669 | ðŸ’¬ 10904\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n2wosj/people_who_used_the_internet_before_2001_what_did/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What's the most pointless thing youâ€™ve spent hours doing, only to realize it was completely useless?\n",
      "ðŸ‘ 5613 | ðŸ’¬ 2958\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n29otv/whats_the_most_pointless_thing_youve_spent_hours/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What animated TV show is a 10/10?\n",
      "ðŸ‘ 5550 | ðŸ’¬ 7945\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n37wwy/what_animated_tv_show_is_a_1010/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What do you think is the greatest comment in Reddit history?\n",
      "ðŸ‘ 5589 | ðŸ’¬ 1658\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n5chco/what_do_you_think_is_the_greatest_comment_in/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What's the silliest reason you stopped having sex with someone?\n",
      "ðŸ‘ 5076 | ðŸ’¬ 2060\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n4b8b8/whats_the_silliest_reason_you_stopped_having_sex/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: what's a good example of \"half the price, but you use 3Ã— as much\"?\n",
      "ðŸ‘ 4883 | ðŸ’¬ 2092\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n1re8c/whats_a_good_example_of_half_the_price_but_you/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What's an example of NSFW college hazing that you know of?\n",
      "ðŸ‘ 4855 | ðŸ’¬ 1759\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n16wsb/whats_an_example_of_nsfw_college_hazing_that_you/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What are some tricks in bed that everyone should know?\n",
      "ðŸ‘ 4760 | ðŸ’¬ 1751\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n0c76e/what_are_some_tricks_in_bed_that_everyone_should/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: Baby boomers of Reddit, how do you feel about the criticism leveled at your generation for supposedly having destroyed the economy and housing market for the younger generation?\n",
      "ðŸ‘ 4683 | ðŸ’¬ 2819\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n44y5m/baby_boomers_of_reddit_how_do_you_feel_about_the/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: Travelers of Reddit what's a destination that looked amazing online but was completely disappointing in person?\n",
      "ðŸ‘ 4842 | ðŸ’¬ 4760\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n5jshb/travelers_of_reddit_whats_a_destination_that/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: Whatâ€™s the most unhinged, chaotic and downright terrible way to lose weight youâ€™ve ever heard of ?\n",
      "ðŸ‘ 4379 | ðŸ’¬ 3090\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n2ixf1/whats_the_most_unhinged_chaotic_and_downright/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: People who wake up after 1 alarm: How the f*ck do you do it?\n",
      "ðŸ‘ 4468 | ðŸ’¬ 5182\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n5ix3g/people_who_wake_up_after_1_alarm_how_the_fck_do/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What is one thing that the younger generation believes that is bullshit?\n",
      "ðŸ‘ 4125 | ðŸ’¬ 3819\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n1hewq/what_is_one_thing_that_the_younger_generation/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What job requires a PhD and pays poorly but most people donâ€™t realize it?\n",
      "ðŸ‘ 3995 | ðŸ’¬ 835\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n4hxe8/what_job_requires_a_phd_and_pays_poorly_but_most/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What screams â€œIâ€™m a horrible spouseâ€?\n",
      "ðŸ‘ 3949 | ðŸ’¬ 1499\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n0qaea/what_screams_im_a_horrible_spouse/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What is an experience that you had which you only see in porn movies?\n",
      "ðŸ‘ 3769 | ðŸ’¬ 1956\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n1cgvx/what_is_an_experience_that_you_had_which_you_only/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What screams â€œIâ€™m a bad parentâ€?\n",
      "ðŸ‘ 3625 | ðŸ’¬ 4054\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n094vk/what_screams_im_a_bad_parent/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: You have a 5 hour drive, you can only listen to music from one band.  Who do you listen to?\n",
      "ðŸ‘ 3210 | ðŸ’¬ 9967\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n2qin4/you_have_a_5_hour_drive_you_can_only_listen_to/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What is the most infamous thing that has occured on Reddit that made the world news?\n",
      "ðŸ‘ 3176 | ðŸ’¬ 1369\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n0eguz/what_is_the_most_infamous_thing_that_has_occured/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What is a historical inaccuracy thats extremely commonly said that really annoys you?\n",
      "ðŸ‘ 3181 | ðŸ’¬ 2211\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n3gfv6/what_is_a_historical_inaccuracy_thats_extremely/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: Which founder would be horrified with the current activities of the company they started?\n",
      "ðŸ‘ 2873 | ðŸ’¬ 1450\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n0m5bi/which_founder_would_be_horrified_with_the_current/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: Which luxury brand is basically just overpriced junk with good marketing?\n",
      "ðŸ‘ 2844 | ðŸ’¬ 2590\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n07pl4/which_luxury_brand_is_basically_just_overpriced/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: Have you ever seen a stranger nude by accident? If so, how did it happen?\n",
      "ðŸ‘ 2758 | ðŸ’¬ 1044\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n4tvw9/have_you_ever_seen_a_stranger_nude_by_accident_if/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What are you slowly realizing as you get older ?\n",
      "ðŸ‘ 2637 | ðŸ’¬ 1432\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n2nz7p/what_are_you_slowly_realizing_as_you_get_older/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What are some good \"you have no concept of time\" facts?\n",
      "ðŸ‘ 2595 | ðŸ’¬ 882\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n233kf/what_are_some_good_you_have_no_concept_of_time/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What is the biggest waste of money that people still defend?\n",
      "ðŸ‘ 2564 | ðŸ’¬ 4129\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n0z2t1/what_is_the_biggest_waste_of_money_that_people/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: Naturally thin people, what do you guys eat in a day?\n",
      "ðŸ‘ 2313 | ðŸ’¬ 2877\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n2aok5/naturally_thin_people_what_do_you_guys_eat_in_a/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: Whatâ€™s the most obvious case of a company ruining their own product on purpose so youâ€™d have to keep buying replacements?\n",
      "ðŸ‘ 2286 | ðŸ’¬ 1337\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n20db2/whats_the_most_obvious_case_of_a_company_ruining/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: People who left their home countries due to political unrest, when did you realize it was time to go?\n",
      "ðŸ‘ 2080 | ðŸ’¬ 566\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n43qm3/people_who_left_their_home_countries_due_to/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What is the creepiest real-life story you know that still sends chills down your spine?\n",
      "ðŸ‘ 2394 | ðŸ’¬ 793\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n5v9vr/what_is_the_creepiest_reallife_story_you_know/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: Whatâ€™s a warning sign society is ignoring right now?\n",
      "ðŸ‘ 1891 | ðŸ’¬ 1683\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n0kgny/whats_a_warning_sign_society_is_ignoring_right_now/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What's your nsfw secret you would take to your grave?\n",
      "ðŸ‘ 1881 | ðŸ’¬ 2094\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n0dieh/whats_your_nsfw_secret_you_would_take_to_your/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: People who grew up poor but are now financially stable, what is a \"poverty habit\" you can't seem to shake?\n",
      "ðŸ‘ 1870 | ðŸ’¬ 1435\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n4jun1/people_who_grew_up_poor_but_are_now_financially/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What feels amazing that's NOT sexual?\n",
      "ðŸ‘ 1822 | ðŸ’¬ 3042\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n1z4xn/what_feels_amazing_thats_not_sexual/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: guys of reddit, whatâ€™s an underrated struggle thatâ€™s unique to being a guy?\n",
      "ðŸ‘ 1786 | ðŸ’¬ 1494\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n1spwu/guys_of_reddit_whats_an_underrated_struggle_thats/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What are you tired of pretending is normal?\n",
      "ðŸ‘ 1640 | ðŸ’¬ 1778\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n4vp8x/what_are_you_tired_of_pretending_is_normal/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: Atheists of Reddit, what is the biggest reason as to why you don't believe in god or an afterlife?\n",
      "ðŸ‘ 1547 | ðŸ’¬ 8705\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n3aq1y/atheists_of_reddit_what_is_the_biggest_reason_as/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: Parents, how often do you have sex and where do you find the time to have it?\n",
      "ðŸ‘ 1579 | ðŸ’¬ 494\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n5jihv/parents_how_often_do_you_have_sex_and_where_do/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "âœ… Saved long stories to long_stories.json\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "\n",
    "load_dotenv(find_dotenv(usecwd=True), override=True)\n",
    "\n",
    "client_secret = os.getenv(\"CLIENT_SECRET\")\n",
    "user_id = os.getenv(\"USER_ID\")\n",
    "\n",
    "# Connect to Reddit\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.getenv(\"CLIENT_ID\"),\n",
    "    client_secret=os.getenv(\"CLIENT_SECRET\"),\n",
    "    user_agent= USER_AGENT\n",
    ")\n",
    "\n",
    "def fetch_long_stories(subreddit_name=\"AskReddit\", limit=20, min_score=5000, min_length=500):\n",
    "    \"\"\"\n",
    "    Pull long, high-engagement posts from a subreddit.\n",
    "    \n",
    "    Args:\n",
    "        subreddit_name: which subreddit to pull from\n",
    "        limit: how many posts to scan\n",
    "        min_score: minimum upvotes required\n",
    "        min_length: minimum length of the body text (characters)\n",
    "    \"\"\"\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    stories = []\n",
    "\n",
    "    print(f\"\\nðŸ“š Long high-engagement posts from r/{subreddit_name}:\")\n",
    "    for post in subreddit.top(time_filter=\"week\", limit=limit):  # top posts of the week\n",
    "        if post.score >= min_score and len(post.selftext) >= min_length:\n",
    "            story = {\n",
    "                \"title\": post.title,\n",
    "                \"score\": post.score,\n",
    "                \"comments\": post.num_comments,\n",
    "                \"url\": \"https://reddit.com\" + post.permalink,\n",
    "                \"text\": post.selftext\n",
    "            }\n",
    "            stories.append(story)\n",
    "            print(f\"\\nTitle: {story['title']}\")\n",
    "            print(f\"ðŸ‘ {story['score']} | ðŸ’¬ {story['comments']}\")\n",
    "            print(f\"ðŸ”— {story['url']}\")\n",
    "            print(f\"ðŸ“ Preview:\\n{story['text'][:400]}...\\n\")\n",
    "\n",
    "    return stories\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    long_posts = fetch_long_stories(limit=50, min_score=0, min_length=0)\n",
    "\n",
    "    # Save to file\n",
    "    import json\n",
    "    with open(\"long_stories.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(long_posts, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(\"\\nâœ… Saved long stories to long_stories.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7836f3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Mining r/AmItheAsshole ===\n",
      "  [1/10] AITA - Do not want a service dog to participate in my wedding....  (ðŸ‘ 10409 | ðŸ’¬ 9225)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 200\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    194\u001b[39m     \u001b[38;5;66;03m# Example 1: specifically r/AskReddit\u001b[39;00m\n\u001b[32m    195\u001b[39m     \u001b[38;5;66;03m# target = [\"AskReddit\"]\u001b[39;00m\n\u001b[32m    196\u001b[39m \n\u001b[32m    197\u001b[39m     \u001b[38;5;66;03m# Example 2: drama-heavy mix\u001b[39;00m\n\u001b[32m    198\u001b[39m     target = DRAMA_SUBREDDITS + [\u001b[33m\"\u001b[39m\u001b[33mAskReddit\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     best = \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     \u001b[38;5;66;03m# Save\u001b[39;00m\n\u001b[32m    203\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(OUTFILE, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 179\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(subreddits)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(posts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpost.title[:\u001b[32m90\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...  (ðŸ‘ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpost.score\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | ðŸ’¬ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpost.num_comments\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     top_comments = \u001b[43mfetch_best_comments_for_submission\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     results.extend(top_comments)\n\u001b[32m    181\u001b[39m     time.sleep(\u001b[32m1.2\u001b[39m)  \u001b[38;5;66;03m# be gentle to rate limits\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 115\u001b[39m, in \u001b[36mfetch_best_comments_for_submission\u001b[39m\u001b[34m(submission)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# Ensure best/top order\u001b[39;00m\n\u001b[32m    114\u001b[39m submission.comment_sort = \u001b[33m\"\u001b[39m\u001b[33mtop\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m \u001b[43msubmission\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcomments\u001b[49m.replace_more(limit=\u001b[32m0\u001b[39m)  \u001b[38;5;66;03m# expand all \"MoreComments\"\u001b[39;00m\n\u001b[32m    117\u001b[39m collected = []\n\u001b[32m    118\u001b[39m count_scanned = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/content-farm/.venv/lib/python3.13/site-packages/praw/models/reddit/base.py:38\u001b[39m, in \u001b[36mRedditBase.__getattr__\u001b[39m\u001b[34m(self, attribute)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return the value of ``attribute``.\"\"\"\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m attribute.startswith(\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fetched:\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attribute)\n\u001b[32m     40\u001b[39m msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattribute\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/content-farm/.venv/lib/python3.13/site-packages/praw/models/reddit/submission.py:726\u001b[39m, in \u001b[36mSubmission._fetch\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    725\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_fetch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m726\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fetch_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    727\u001b[39m     submission_listing, comment_listing = data\n\u001b[32m    728\u001b[39m     comment_listing = Listing(\u001b[38;5;28mself\u001b[39m._reddit, _data=comment_listing[\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/content-farm/.venv/lib/python3.13/site-packages/praw/models/reddit/submission.py:744\u001b[39m, in \u001b[36mSubmission._fetch_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    742\u001b[39m params.update(\u001b[38;5;28mself\u001b[39m._additional_fetch_params.copy())\n\u001b[32m    743\u001b[39m path = API_PATH[name].format(**fields)\n\u001b[32m--> \u001b[39m\u001b[32m744\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reddit\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/content-farm/.venv/lib/python3.13/site-packages/praw/util/deprecate_args.py:46\u001b[39m, in \u001b[36m_deprecate_args.<locals>.wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     39\u001b[39m     arg_string = _generate_arg_string(_old_args[: \u001b[38;5;28mlen\u001b[39m(args)])\n\u001b[32m     40\u001b[39m     warn(\n\u001b[32m     41\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPositional arguments for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m will no longer be\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     42\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m supported in PRAW 8.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mCall this function with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     43\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m     44\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m     45\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_old_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/content-farm/.venv/lib/python3.13/site-packages/praw/reddit.py:963\u001b[39m, in \u001b[36mReddit.request\u001b[39m\u001b[34m(self, data, files, json, method, params, path)\u001b[39m\n\u001b[32m    961\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ClientException(msg)\n\u001b[32m    962\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m963\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_core\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m BadRequest \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[32m    972\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/content-farm/.venv/lib/python3.13/site-packages/prawcore/sessions.py:328\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, path, data, files, json, params, timeout)\u001b[39m\n\u001b[32m    326\u001b[39m     json[\u001b[33m\"\u001b[39m\u001b[33mapi_type\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mjson\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    327\u001b[39m url = urljoin(\u001b[38;5;28mself\u001b[39m._requestor.oauth_url, path)\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_with_retries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/content-farm/.venv/lib/python3.13/site-packages/prawcore/sessions.py:234\u001b[39m, in \u001b[36mSession._request_with_retries\u001b[39m\u001b[34m(self, data, files, json, method, params, timeout, url, retry_strategy_state)\u001b[39m\n\u001b[32m    232\u001b[39m retry_strategy_state.sleep()\n\u001b[32m    233\u001b[39m \u001b[38;5;28mself\u001b[39m._log_request(data, method, params, url)\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m response, saved_exception = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry_strategy_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m do_retry = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m response.status_code == codes[\u001b[33m\"\u001b[39m\u001b[33munauthorized\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/content-farm/.venv/lib/python3.13/site-packages/prawcore/sessions.py:186\u001b[39m, in \u001b[36mSession._make_request\u001b[39m\u001b[34m(self, data, files, json, method, params, retry_strategy_state, timeout, url)\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_make_request\u001b[39m(\n\u001b[32m    175\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    176\u001b[39m     data: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]],\n\u001b[32m   (...)\u001b[39m\u001b[32m    183\u001b[39m     url: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    184\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[Response, \u001b[38;5;28;01mNone\u001b[39;00m] | \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m]:\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m         response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_rate_limiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_requestor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_set_header_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m            \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m            \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m         log.debug(\n\u001b[32m    199\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mResponse: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m bytes) (rst-\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m:rem-\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m:used-\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m ratelimit) at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    200\u001b[39m             response.status_code,\n\u001b[32m   (...)\u001b[39m\u001b[32m    205\u001b[39m             time.time(),\n\u001b[32m    206\u001b[39m         )\n\u001b[32m    207\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/content-farm/.venv/lib/python3.13/site-packages/prawcore/rate_limit.py:47\u001b[39m, in \u001b[36mRateLimiter.call\u001b[39m\u001b[34m(self, request_function, set_header_callback, *args, **kwargs)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28mself\u001b[39m.delay()\n\u001b[32m     46\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m] = set_header_callback()\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m response = \u001b[43mrequest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;28mself\u001b[39m.update(response.headers)\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/content-farm/.venv/lib/python3.13/site-packages/prawcore/requestor.py:68\u001b[39m, in \u001b[36mRequestor.request\u001b[39m\u001b[34m(self, timeout, *args, **kwargs)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Issue the HTTP request capturing any errors that may occur.\"\"\"\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_http\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:  \u001b[38;5;66;03m# noqa: BLE001\u001b[39;00m\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RequestException(exc, args, kwargs) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/content-farm/.venv/lib/python3.13/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/content-farm/.venv/lib/python3.13/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/content-farm/.venv/lib/python3.13/site-packages/requests/adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    641\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/content-farm/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/content-farm/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/content-farm/.venv/lib/python3.13/site-packages/urllib3/connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    568\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:1428\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1426\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1427\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1428\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1429\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1430\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    333\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    294\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1304\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1300\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1301\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1302\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1303\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1305\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1138\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# pull_best_comments.py\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "import praw\n",
    "from prawcore.exceptions import RequestException, ResponseException, ServerError\n",
    "\n",
    "# ----------------------------\n",
    "# SETTINGS\n",
    "# ----------------------------\n",
    "DRAMA_SUBREDDITS = [\n",
    "    \"AmItheAsshole\", \"BestofRedditorUpdates\", \"relationship_advice\",\n",
    "    \"TrueOffMyChest\", \"relationships\", \"JustNoMIL\", \"entitledparents\",\n",
    "    \"OffMyChest\", \"survivinginfidelity\"\n",
    "]\n",
    "# You can also include 'all' in the list, or just target \"AskReddit\" specifically\n",
    "\n",
    "DRAMA_KEYWORDS = [\n",
    "    \"divorce\",\"cheat\",\"affair\",\"wedding\",\"engagement\",\"pregnant\",\"custody\",\n",
    "    \"family\",\"mother-in-law\",\"entitled\",\"betray\",\"gaslight\",\"toxic\",\"lawsuit\",\n",
    "    \"will\",\"inherit\",\"ex\",\"backfire\",\"secret\",\"lied\",\"scammed\",\"abusive\"\n",
    "]\n",
    "DRAMA_RE = re.compile(r\"\\b(\" + \"|\".join(re.escape(k) for k in DRAMA_KEYWORDS) + r\")\\b\", re.I)\n",
    "\n",
    "# Post filters\n",
    "MIN_POST_SCORE = 2000          # baseline quality gate\n",
    "MIN_POST_COMMENTS = 150\n",
    "POSTS_PER_SUBREDDIT = 10       # how many candidate posts to scan per subreddit\n",
    "POST_TIME_FILTER = \"week\"      # 'day' | 'week' | 'month' | 'year' | 'all'\n",
    "\n",
    "# Comment filters\n",
    "MIN_COMMENT_SCORE = 200\n",
    "MIN_COMMENT_LENGTH = 400       # characters\n",
    "MAX_COMMENTS_TO_SCAN = 600     # safety cap per post to limit API work\n",
    "TOP_COMMENTS_PER_POST = 8      # how many to keep per post\n",
    "\n",
    "# Output\n",
    "OUTFILE = \"best_comments.json\"\n",
    "\n",
    "# ----------------------------\n",
    "# REDDIT CLIENT\n",
    "# ----------------------------\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "\n",
    "load_dotenv(find_dotenv(usecwd=True), override=True)\n",
    "\n",
    "client_secret = os.getenv(\"CLIENT_SECRET\")\n",
    "user_id = os.getenv(\"USER_ID\")\n",
    "\n",
    "# Connect to Reddit\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.getenv(\"CLIENT_ID\"),\n",
    "    client_secret=os.getenv(\"CLIENT_SECRET\"),\n",
    "    user_agent= USER_AGENT\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# HELPERS\n",
    "# ----------------------------\n",
    "def looks_removed(text: str) -> bool:\n",
    "    t = (text or \"\").strip().lower()\n",
    "    return t in (\"[removed]\", \"[deleted]\")\n",
    "\n",
    "def comment_engagement_score(c) -> float:\n",
    "    \"\"\"\n",
    "    A simple, tunable scoring function.\n",
    "    We reward: upvotes, awards, replies, controversy, length, and drama keywords.\n",
    "    \"\"\"\n",
    "    score = getattr(c, \"score\", 0) or 0\n",
    "    awards = getattr(c, \"total_awards_received\", 0) or 0\n",
    "    controversiality = getattr(c, \"controversiality\", 0) or 0\n",
    "    body = getattr(c, \"body\", \"\") or \"\"\n",
    "    length = len(body)\n",
    "\n",
    "    # Direct replies count (only loaded if tree expanded; still a decent proxy)\n",
    "    try:\n",
    "        direct_replies = len(c.replies)\n",
    "    except Exception:\n",
    "        direct_replies = 0\n",
    "\n",
    "    # Base: votes + awards + replies\n",
    "    s = score + 50 * awards + 4 * direct_replies\n",
    "\n",
    "    # Heated debates get a bump\n",
    "    s += 120 * controversiality\n",
    "\n",
    "    # Reward long-form (cap the boost)\n",
    "    s += min(length // 300, 6) * 25  # +25 per 300 chars up to 6 chunks\n",
    "\n",
    "    # Drama keywords bonus\n",
    "    if DRAMA_RE.search(body):\n",
    "        s += 120\n",
    "\n",
    "    # OP comments are often interesting but not always ideal for â€œcommentaryâ€ videos.\n",
    "    # If you want to favor OPâ€™s clarifications, uncomment:\n",
    "    # if getattr(c, \"is_submitter\", False):\n",
    "    #     s += 40\n",
    "\n",
    "    return float(s)\n",
    "\n",
    "def fetch_candidate_submissions(subreddit_name: str) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Pull high-engagement posts to mine for comments.\n",
    "    We use .top(time_filter=POST_TIME_FILTER) to bias toward proven engagement.\n",
    "    \"\"\"\n",
    "    sub = reddit.subreddit(subreddit_name)\n",
    "    candidates = []\n",
    "    # You can mix sources; here we use top-week. You can also try rising() for â€œbreaking.â€\n",
    "    for post in sub.top(time_filter=POST_TIME_FILTER, limit=POSTS_PER_SUBREDDIT * 2):\n",
    "        if post.score >= MIN_POST_SCORE and post.num_comments >= MIN_POST_COMMENTS and not post.stickied:\n",
    "            candidates.append(post)\n",
    "            if len(candidates) >= POSTS_PER_SUBREDDIT:\n",
    "                break\n",
    "    return candidates\n",
    "\n",
    "def fetch_best_comments_for_submission(submission) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Expand comment tree, filter, score, and return the best comments.\n",
    "    \"\"\"\n",
    "    # Ensure best/top order\n",
    "    submission.comment_sort = \"top\"\n",
    "    submission.comments.replace_more(limit=0)  # expand all \"MoreComments\"\n",
    "\n",
    "    collected = []\n",
    "    count_scanned = 0\n",
    "\n",
    "    # Flattened traversal (ordered by current sort)\n",
    "    for c in submission.comments.list():\n",
    "        if count_scanned >= MAX_COMMENTS_TO_SCAN:\n",
    "            break\n",
    "        count_scanned += 1\n",
    "\n",
    "        # Skip non-Comment objects defensively\n",
    "        if c.__class__.__name__ != \"Comment\":\n",
    "            continue\n",
    "\n",
    "        body = getattr(c, \"body\", \"\") or \"\"\n",
    "        if looks_removed(body):\n",
    "            continue\n",
    "        if len(body) < MIN_COMMENT_LENGTH:\n",
    "            continue\n",
    "        if getattr(c, \"score\", 0) < MIN_COMMENT_SCORE:\n",
    "            continue\n",
    "        if getattr(c, \"stickied\", False):\n",
    "            continue  # skip auto-mod stickies\n",
    "\n",
    "        s = comment_engagement_score(c)\n",
    "\n",
    "        collected.append({\n",
    "            \"submission_id\": submission.id,\n",
    "            \"submission_title\": submission.title,\n",
    "            \"submission_url\": \"https://reddit.com\" + submission.permalink,\n",
    "            \"submission_score\": submission.score,\n",
    "            \"submission_num_comments\": submission.num_comments,\n",
    "\n",
    "            \"comment_id\": c.id,\n",
    "            \"comment_permalink\": \"https://reddit.com\" + c.permalink,\n",
    "            \"author\": str(getattr(c, \"author\", None)) if getattr(c, \"author\", None) else \"[deleted]\",\n",
    "            \"score\": c.score,\n",
    "            \"awards\": getattr(c, \"total_awards_received\", 0) or 0,\n",
    "            \"controversiality\": getattr(c, \"controversiality\", 0) or 0,\n",
    "            \"is_submitter\": getattr(c, \"is_submitter\", False),\n",
    "            \"direct_replies\": len(c.replies) if hasattr(c, \"replies\") else 0,\n",
    "            \"length\": len(body),\n",
    "            \"body\": body,\n",
    "            \"engagement_score\": s\n",
    "        })\n",
    "\n",
    "    # Sort and take top N\n",
    "    collected.sort(key=lambda x: x[\"engagement_score\"], reverse=True)\n",
    "    return collected[:TOP_COMMENTS_PER_POST]\n",
    "\n",
    "def run(subreddits: List[str]) -> List[Dict[str, Any]]:\n",
    "    results = []\n",
    "    for name in subreddits:\n",
    "        print(f\"\\n=== Mining r/{name} ===\")\n",
    "        try:\n",
    "            posts = fetch_candidate_submissions(name)\n",
    "        except (RequestException, ResponseException, ServerError) as e:\n",
    "            print(f\"Skipping r/{name} due to API error: {e}\")\n",
    "            continue\n",
    "\n",
    "        for i, post in enumerate(posts, 1):\n",
    "            print(f\"  [{i}/{len(posts)}] {post.title[:90]}...  (ðŸ‘ {post.score} | ðŸ’¬ {post.num_comments})\")\n",
    "            try:\n",
    "                top_comments = fetch_best_comments_for_submission(post)\n",
    "                results.extend(top_comments)\n",
    "                time.sleep(1.2)  # be gentle to rate limits\n",
    "            except (RequestException, ResponseException, ServerError) as e:\n",
    "                print(f\"    Failed on comments for {post.id}: {e}\")\n",
    "                time.sleep(1.5)\n",
    "\n",
    "        # short pause between subreddits\n",
    "        time.sleep(1.5)\n",
    "\n",
    "    # Global sort to surface absolute bangers\n",
    "    results.sort(key=lambda x: x[\"engagement_score\"], reverse=True)\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example 1: specifically r/AskReddit\n",
    "    # target = [\"AskReddit\"]\n",
    "\n",
    "    # Example 2: drama-heavy mix\n",
    "    target = DRAMA_SUBREDDITS + [\"AskReddit\"]\n",
    "\n",
    "    best = run(target)\n",
    "\n",
    "    # Save\n",
    "    with open(OUTFILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(best, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\nâœ… Saved {len(best)} high-engagement, long comments â†’ {OUTFILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668e4ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Mining r/AmItheAsshole ===\n",
      "  > Scanning posts from r/AmItheAsshole (top/week) â€¦\n",
      "  > Found 10 long story posts in r/AmItheAsshole\n",
      "  [1/10] AITA - Do not want a service dog to participate in my wedding....  (ðŸ‘ 10612 | ðŸ’¬ 9347)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n2/rstg0c3j7hv2shgc8szmsj2r0000gp/T/ipykernel_60185/2257734352.py:181: UserWarning: The comments for this submission have already been fetched, so the updated comment_sort will not have any effect.\n",
      "  submission.comment_sort = sort\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [2/10] AITA for not changing my annual backyard party plans for my boyfriend and his kids?...  (ðŸ‘ 9298 | ðŸ’¬ 1206)\n",
      "  [3/10] AITA for telling my sister she wasn't always the chosen one?...  (ðŸ‘ 8095 | ðŸ’¬ 1699)\n",
      "  [4/10] AITA Tension After My Father Passed Away and I had to Leave 36 Hours Later Because of My I...  (ðŸ‘ 7046 | ðŸ’¬ 1548)\n",
      "  [5/10] AITA for asking my girlfriend to tell me what flavour of smoothie she wanted?...  (ðŸ‘ 6081 | ðŸ’¬ 687)\n"
     ]
    }
   ],
   "source": [
    "# pull_best_comments.py\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "import praw\n",
    "from prawcore.exceptions import RequestException, ResponseException, ServerError, Forbidden\n",
    "\n",
    "# ----------------------------\n",
    "# SETTINGS\n",
    "# ----------------------------\n",
    "DRAMA_SUBREDDITS = [\n",
    "    \"AmItheAsshole\", \"BestofRedditorUpdates\", \"relationship_advice\",\n",
    "    \"TrueOffMyChest\", \"relationships\", \"JustNoMIL\", \"entitledparents\",\n",
    "    \"OffMyChest\", \"survivinginfidelity\"\n",
    "]\n",
    "# You can also include 'all' in the list, or just target \"AskReddit\" specifically\n",
    "\n",
    "DRAMA_KEYWORDS = [\n",
    "    \"divorce\",\"cheat\",\"affair\",\"wedding\",\"engagement\",\"pregnant\",\"custody\",\n",
    "    \"family\",\"mother-in-law\",\"entitled\",\"betray\",\"gaslight\",\"toxic\",\"lawsuit\",\n",
    "    \"will\",\"inherit\",\"ex\",\"backfire\",\"secret\",\"lied\",\"scammed\",\"abusive\",\"update\",\"boundary\"\n",
    "]\n",
    "DRAMA_RE = re.compile(r\"\\b(\" + \"|\".join(re.escape(k) for k in DRAMA_KEYWORDS) + r\")\\b\", re.I)\n",
    "\n",
    "# Post filters (for **posts** weâ€™ll also use these gates)\n",
    "MIN_POST_SCORE = 2000          # baseline quality gate\n",
    "MIN_POST_COMMENTS = 150\n",
    "POSTS_PER_SUBREDDIT = 10       # how many candidate posts to scan per subreddit\n",
    "POST_TIME_FILTER = \"week\"      # 'day' | 'week' | 'month' | 'year' | 'all'\n",
    "\n",
    "# Extra post gate for long-form stories\n",
    "MIN_POST_LENGTH = 700          # chars in selftext (longer = more likely a full story)\n",
    "\n",
    "# Comment filters\n",
    "MIN_COMMENT_SCORE = 200\n",
    "MIN_COMMENT_LENGTH = 400       # characters\n",
    "MAX_COMMENTS_TO_SCAN = 600     # safety cap per post to limit API work\n",
    "TOP_COMMENTS_PER_POST = 8      # how many to keep per post\n",
    "\n",
    "# Output\n",
    "OUTFILE = \"best_comments.json\"             # (same name as your original)\n",
    "POSTS_OUTFILE = \"best_posts.json\"          # new: long story posts (title + selftext)\n",
    "COMBINED_OUTFILE = \"stories_with_comments.json\"  # new: post + its top comments\n",
    "\n",
    "# ----------------------------\n",
    "# REDDIT CLIENT\n",
    "# (uses your existing globals: CLIENT_ID / CLIENT_SECRET / USER_AGENT)\n",
    "# ----------------------------\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "\n",
    "load_dotenv(find_dotenv(usecwd=True), override=True)\n",
    "\n",
    "client_secret = os.getenv(\"CLIENT_SECRET\")\n",
    "user_id = os.getenv(\"USER_ID\")\n",
    "\n",
    "# Connect to Reddit\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.getenv(\"CLIENT_ID\"),\n",
    "    client_secret=os.getenv(\"CLIENT_SECRET\"),\n",
    "    user_agent= USER_AGENT\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# HELPERS\n",
    "# ----------------------------\n",
    "def looks_removed(text: str) -> bool:\n",
    "    t = (text or \"\").strip().lower()\n",
    "    return t in (\"[removed]\", \"[deleted]\")\n",
    "\n",
    "def comment_engagement_score(c) -> float:\n",
    "    \"\"\"\n",
    "    A simple, tunable scoring function.\n",
    "    We reward: upvotes, awards, replies, controversy, length, and drama keywords.\n",
    "    \"\"\"\n",
    "    score = getattr(c, \"score\", 0) or 0\n",
    "    awards = getattr(c, \"total_awards_received\", 0) or 0\n",
    "    controversiality = getattr(c, \"controversiality\", 0) or 0\n",
    "    body = getattr(c, \"body\", \"\") or \"\"\n",
    "    length = len(body)\n",
    "\n",
    "    # Direct replies count (only loaded if tree expanded; still a decent proxy)\n",
    "    try:\n",
    "        direct_replies = len(c.replies)\n",
    "    except Exception:\n",
    "        direct_replies = 0\n",
    "\n",
    "    # Base: votes + awards + replies\n",
    "    s = score + 50 * awards + 4 * direct_replies\n",
    "\n",
    "    # Heated debates get a bump\n",
    "    s += 120 * controversiality\n",
    "\n",
    "    # Reward long-form (cap the boost)\n",
    "    s += min(length // 300, 6) * 25  # +25 per 300 chars up to 6 chunks\n",
    "\n",
    "    # Drama keywords bonus\n",
    "    if DRAMA_RE.search(body):\n",
    "        s += 120\n",
    "\n",
    "    # OP comments are often interesting but not always ideal for â€œcommentaryâ€ videos.\n",
    "    # If you want to favor OPâ€™s clarifications, uncomment:\n",
    "    # if getattr(c, \"is_submitter\", False):\n",
    "    #     s += 40\n",
    "\n",
    "    return float(s)\n",
    "\n",
    "# ----------------------------\n",
    "# POSTS: discovery (long story posts)\n",
    "# ----------------------------\n",
    "def fetch_long_story_posts(subreddit_name: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Discover long, engaged **posts** (selftext stories) from a subreddit.\n",
    "    Uses .top() for the configured POST_TIME_FILTER and applies your post gates.\n",
    "    \"\"\"\n",
    "    sub = reddit.subreddit(subreddit_name)\n",
    "    results = []\n",
    "\n",
    "    print(f\"  > Scanning posts from r/{subreddit_name} (top/{POST_TIME_FILTER}) â€¦\")\n",
    "    # Pull a bit wider than POSTS_PER_SUBREDDIT so filters can cut down to ~target\n",
    "    limit = max(POSTS_PER_SUBREDDIT * 5, 30)\n",
    "    for post in sub.top(time_filter=POST_TIME_FILTER, limit=limit):\n",
    "        try:\n",
    "            if post.stickied:\n",
    "                continue\n",
    "            if not getattr(post, \"is_self\", False):\n",
    "                continue\n",
    "\n",
    "            text = (post.selftext or \"\").strip()\n",
    "            if not text or looks_removed(text):\n",
    "                continue\n",
    "            if len(text) < MIN_POST_LENGTH:\n",
    "                continue\n",
    "            if post.score < MIN_POST_SCORE or post.num_comments < MIN_POST_COMMENTS:\n",
    "                continue\n",
    "\n",
    "            row = {\n",
    "                \"id\": post.id,\n",
    "                \"subreddit\": str(post.subreddit),\n",
    "                \"title\": post.title,\n",
    "                \"url\": \"https://reddit.com\" + post.permalink,\n",
    "                \"score\": int(post.score),\n",
    "                \"num_comments\": int(post.num_comments),\n",
    "                \"upvote_ratio\": getattr(post, \"upvote_ratio\", None),\n",
    "                \"created_utc\": float(post.created_utc),\n",
    "                \"length\": len(text),\n",
    "                \"selftext\": text\n",
    "            }\n",
    "            results.append(row)\n",
    "\n",
    "            if len(results) >= POSTS_PER_SUBREDDIT:\n",
    "                break\n",
    "        except Exception:\n",
    "            # Skip weird/unexpected objects\n",
    "            continue\n",
    "\n",
    "    # Rank posts very simply: score + 2*comments (+ keyword bump)\n",
    "    def post_rank_key(p):\n",
    "        base = p[\"score\"] + 2 * p[\"num_comments\"]\n",
    "        if DRAMA_RE.search(p[\"title\"]) or DRAMA_RE.search(p[\"selftext\"]):\n",
    "            base += 200\n",
    "        return base\n",
    "\n",
    "    results.sort(key=post_rank_key, reverse=True)\n",
    "    print(f\"  > Found {len(results)} long story posts in r/{subreddit_name}\")\n",
    "    return results\n",
    "\n",
    "# ----------------------------\n",
    "# COMMENTS: mining (top + controversial)\n",
    "# ----------------------------\n",
    "def fetch_best_comments_for_submission(submission) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Expand comment tree, filter, score, and return the best comments.\n",
    "    We scan **both** 'top' and 'controversial' sorts, with long pauses.\n",
    "    \"\"\"\n",
    "    collected = []\n",
    "    seen_ids = set()\n",
    "    for sort in (\"top\", \"controversial\"):\n",
    "        try:\n",
    "            submission.comment_sort = sort\n",
    "            submission.comments.replace_more(limit=0)  # expand all \"MoreComments\"\n",
    "        except (RequestException, ResponseException, ServerError, Forbidden):\n",
    "            continue\n",
    "\n",
    "        count_scanned = 0\n",
    "        for c in submission.comments.list():\n",
    "            if count_scanned >= MAX_COMMENTS_TO_SCAN:\n",
    "                break\n",
    "            count_scanned += 1\n",
    "\n",
    "            if c.__class__.__name__ != \"Comment\":\n",
    "                continue\n",
    "            if c.id in seen_ids:\n",
    "                continue\n",
    "\n",
    "            body = getattr(c, \"body\", \"\") or \"\"\n",
    "            if looks_removed(body):\n",
    "                continue\n",
    "            if len(body) < MIN_COMMENT_LENGTH:\n",
    "                continue\n",
    "            if getattr(c, \"score\", 0) < MIN_COMMENT_SCORE:\n",
    "                continue\n",
    "            if getattr(c, \"stickied\", False):\n",
    "                continue  # skip auto-mod stickies\n",
    "\n",
    "            s = comment_engagement_score(c)\n",
    "            seen_ids.add(c.id)\n",
    "            collected.append({\n",
    "                \"submission_id\": submission.id,\n",
    "                \"submission_title\": submission.title,\n",
    "                \"submission_url\": \"https://reddit.com\" + submission.permalink,\n",
    "                \"submission_score\": submission.score,\n",
    "                \"submission_num_comments\": submission.num_comments,\n",
    "\n",
    "                \"comment_id\": c.id,\n",
    "                \"comment_permalink\": \"https://reddit.com\" + c.permalink,\n",
    "                \"author\": str(getattr(c, \"author\", None)) if getattr(c, \"author\", None) else \"[deleted]\",\n",
    "                \"score\": int(getattr(c, \"score\", 0) or 0),\n",
    "                \"awards\": int(getattr(c, \"total_awards_received\", 0) or 0),\n",
    "                \"controversiality\": int(getattr(c, \"controversiality\", 0) or 0),\n",
    "                \"is_submitter\": bool(getattr(c, \"is_submitter\", False)),\n",
    "                \"direct_replies\": int(len(c.replies) if hasattr(c, \"replies\") else 0),\n",
    "                \"length\": len(body),\n",
    "                \"body\": body,\n",
    "                \"engagement_score\": s,\n",
    "                \"sort_source\": sort\n",
    "            })\n",
    "\n",
    "        # ~3s rest between sorts\n",
    "        time.sleep(3.0)\n",
    "\n",
    "    # Sort and take top N\n",
    "    collected.sort(key=lambda x: x[\"engagement_score\"], reverse=True)\n",
    "    return collected[:TOP_COMMENTS_PER_POST]\n",
    "\n",
    "# ----------------------------\n",
    "# Orchestrator (keeps same signature)\n",
    "# ----------------------------\n",
    "def run(subreddits: List[str]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    EXACT same call shape as your original: returns a flat list of top comments\n",
    "    and (side effect) also writes posts and combined files.\n",
    "    \"\"\"\n",
    "    all_posts = []\n",
    "    all_comments = []\n",
    "    combined = []\n",
    "\n",
    "    for name in subreddits:\n",
    "        print(f\"\\n=== Mining r/{name} ===\")\n",
    "\n",
    "        # 1) Pull **posts** (long-form stories)\n",
    "        try:\n",
    "            posts = fetch_long_story_posts(name)\n",
    "            all_posts.extend(posts)\n",
    "        except (RequestException, ResponseException, ServerError, Forbidden) as e:\n",
    "            print(f\"Skipping r/{name} due to API error (posts): {e}\")\n",
    "            # 3s rest even on error\n",
    "            time.sleep(3.0)\n",
    "            continue\n",
    "\n",
    "        # 2) For each post, pull **comments**\n",
    "        for i, post in enumerate(posts, 1):\n",
    "            print(f\"  [{i}/{len(posts)}] {post['title'][:90]}...  (ðŸ‘ {post['score']} | ðŸ’¬ {post['num_comments']})\")\n",
    "            try:\n",
    "                subm = reddit.submission(id=post[\"id\"])\n",
    "                top_comments = fetch_best_comments_for_submission(subm)\n",
    "                all_comments.extend(top_comments)\n",
    "                combined.append({\n",
    "                    \"post\": post,\n",
    "                    \"top_comments\": top_comments\n",
    "                })\n",
    "            except (RequestException, ResponseException, ServerError, Forbidden) as e:\n",
    "                print(f\"    Failed on comments for {post['id']}: {e}\")\n",
    "\n",
    "            # ~3s rest between submissions\n",
    "            time.sleep(3.0)\n",
    "\n",
    "        # ~3s rest between subreddits\n",
    "        time.sleep(3.0)\n",
    "\n",
    "    # Sort outputs\n",
    "    all_comments.sort(key=lambda x: x[\"engagement_score\"], reverse=True)\n",
    "\n",
    "    # Persist extra files here (side effects)\n",
    "    try:\n",
    "        with open(POSTS_OUTFILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(all_posts, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"\\nâœ… Saved {len(all_posts)} long story posts â†’ {POSTS_OUTFILE}\")\n",
    "\n",
    "        with open(COMBINED_OUTFILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(combined, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"âœ… Saved {len(combined)} combined records (post + comments) â†’ {COMBINED_OUTFILE}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Failed writing posts/combined files: {e}\")\n",
    "\n",
    "    # Return comments (so your main keeps working identically)\n",
    "    return all_comments\n",
    "\n",
    "# ----------------------------\n",
    "# MAIN (unchanged calling style)\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Example 1: specifically r/AskReddit\n",
    "    # target = [\"AskReddit\"]\n",
    "\n",
    "    # Example 2: drama-heavy mix\n",
    "    target = DRAMA_SUBREDDITS + [\"AskReddit\"]\n",
    "\n",
    "    best = run(target)\n",
    "\n",
    "    # Save (same as your original)\n",
    "    with open(OUTFILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(best, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\nâœ… Saved {len(best)} high-engagement, long comments â†’ {OUTFILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22d5cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.9.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
