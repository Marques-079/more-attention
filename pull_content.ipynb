{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee55cfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "CLIENT_ID = \"rKZcgn75WxMWogN7JNecPw\"\n",
    "CLIENT_SECRET = \"m4N23XQM8Eds4F41I1PI5Z7U4sLS3Q\"\n",
    "USER_AGENT = \"Channel1Scraper:1.0 (by u/AlarmingSignal4018)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6511271e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¥ Top 20 rising posts in r/all ðŸ”¥\n",
      "Polish tennis player found the kid who got his cap stolen by the Scumbag | ðŸ‘ 953 | ðŸ’¬ 33\n",
      "Leaders of nearly 3 billion people met yesterday. | ðŸ‘ 2332 | ðŸ’¬ 376\n",
      "What? ðŸ¤£ðŸ¤£ðŸ¤£ | ðŸ‘ 163 | ðŸ’¬ 54\n",
      "Thhe greatest prank of all time without question | ðŸ‘ 408 | ðŸ’¬ 24\n",
      "Ouch Charlie Kirk got told! | ðŸ‘ 341 | ðŸ’¬ 10\n",
      "Maybe Maybe Maybe | ðŸ‘ 135 | ðŸ’¬ 32\n",
      "Hilarious - Mexico will pay the funeral | ðŸ‘ 1009 | ðŸ’¬ 19\n",
      "Rulediohead | ðŸ‘ 196 | ðŸ’¬ 6\n",
      "Texas different.. | ðŸ‘ 105 | ðŸ’¬ 11\n",
      "This orange I just picked | ðŸ‘ 912 | ðŸ’¬ 46\n",
      "Reacher' Star Alan Ritchson Goes Scorched Earth On Trump â€” Can't Believe Christians Support Him: â€œTrump is a rapist and a con man, and yet the entire Christian church seems to treat him like heâ€™s their poster child, and itâ€™s unreal. | ðŸ‘ 227 | ðŸ’¬ 20\n",
      "Brooo lol | ðŸ‘ 166 | ðŸ’¬ 9\n",
      "There is no movie format on earth vlc cannot play. | ðŸ‘ 125 | ðŸ’¬ 29\n",
      "Polish tennis player found the kid who got his signed cap taken by the CEO. | ðŸ‘ 109 | ðŸ’¬ 9\n",
      "On a video about keeping kids safe from predators online | ðŸ‘ 98 | ðŸ’¬ 24\n",
      "Rep. Leviste wants to lower the VAT because the Philippines has the highest VAT in SEA. According to him its one of the most regressive form of taxation because its predominantly paid by the poor but he also wants to introduce wealth tax especially those who buy properties in places like Forbes Park | ðŸ‘ 214 | ðŸ’¬ 42\n",
      "Nonchalant Bear Attack!! | ðŸ‘ 107 | ðŸ’¬ 22\n",
      "Be the spark that fuels someoneâ€™s determination | ðŸ‘ 221 | ðŸ’¬ 9\n",
      "ðŸ’ªðŸ’ªðŸ’ª | ðŸ‘ 89 | ðŸ’¬ 1\n",
      "Everyone say hi to Stormy. This little guy is 21 this year! | ðŸ‘ 829 | ðŸ’¬ 57\n",
      "\n",
      "âœ… Saved trending stories to trending.json\n"
     ]
    }
   ],
   "source": [
    "# pull_stories.py\n",
    "\n",
    "import praw\n",
    "\n",
    "# Connect to Reddit\n",
    "reddit = praw.Reddit(\n",
    "    client_id=CLIENT_ID,\n",
    "    client_secret=CLIENT_SECRET,\n",
    "    user_agent=USER_AGENT\n",
    ")\n",
    "\n",
    "def fetch_trending(subreddit_name=\"all\", limit=10):\n",
    "    \"\"\"Fetch trending stories from a subreddit (default: all)\"\"\"\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    stories = []\n",
    "\n",
    "    print(f\"\\nðŸ”¥ Top {limit} rising posts in r/{subreddit_name} ðŸ”¥\")\n",
    "    for post in subreddit.rising(limit=limit):\n",
    "        story = {\n",
    "            \"title\": post.title,\n",
    "            \"score\": post.score,\n",
    "            \"comments\": post.num_comments,\n",
    "            \"url\": post.url,\n",
    "            \"text\": post.selftext[:500]  # preview first 500 chars\n",
    "        }\n",
    "        stories.append(story)\n",
    "        print(f\"{story['title']} | ðŸ‘ {story['score']} | ðŸ’¬ {story['comments']}\")\n",
    "\n",
    "    return stories\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trending = fetch_trending(limit=20)\n",
    "\n",
    "    # Save results to JSON\n",
    "    import json\n",
    "    with open(\"trending.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(trending, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(\"\\nâœ… Saved trending stories to trending.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3dbfc075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“š Long high-engagement posts from r/AskReddit:\n",
      "\n",
      "Title: Trump said today: â€œI have the right to do anything I want to do. Iâ€™m the president of the United Statesâ€. People from the US, what are your thoughts on this?\n",
      "ðŸ‘ 57198 | ðŸ’¬ 13671\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n0xxaf/trump_said_today_i_have_the_right_to_do_anything/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What's a ticking time bomb you believe will explode during your lifetime?\n",
      "ðŸ‘ 15282 | ðŸ’¬ 10081\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n03f1i/whats_a_ticking_time_bomb_you_believe_will/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What is a job that pays extremely well but no one realizes it?\n",
      "ðŸ‘ 12513 | ðŸ’¬ 5863\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n4g11a/what_is_a_job_that_pays_extremely_well_but_no_one/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: Older people on Reddit. What is 100% pure bullshit?\n",
      "ðŸ‘ 12105 | ðŸ’¬ 9833\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1mzmcgk/older_people_on_reddit_what_is_100_pure_bullshit/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: To those who have left the MAGA movement, what was your wake-up call?\n",
      "ðŸ‘ 11164 | ðŸ’¬ 6840\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n14pol/to_those_who_have_left_the_maga_movement_what_was/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What would the GOP power struggle look like if Donald Trump dies?\n",
      "ðŸ‘ 8543 | ðŸ’¬ 2930\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n40xp2/what_would_the_gop_power_struggle_look_like_if/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What is a little quirk about your body that you donâ€™t think other people have?\n",
      "ðŸ‘ 7246 | ðŸ’¬ 12255\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n3qqcs/what_is_a_little_quirk_about_your_body_that_you/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What Do You Think Would Be The Condition Of The United States Today if Hillary Clinton Had Become President in January 2017?\n",
      "ðŸ‘ 7002 | ðŸ’¬ 4522\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n4ulw7/what_do_you_think_would_be_the_condition_of_the/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: How would you feel about your partner waking you up just to have sex?\n",
      "ðŸ‘ 6668 | ðŸ’¬ 1805\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n32iaw/how_would_you_feel_about_your_partner_waking_you/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: Men: What's the brutally honest dating advice you wish the women in your life (sisters, daughters, friends, etc.) understood?\n",
      "ðŸ‘ 6356 | ðŸ’¬ 3270\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n3liug/men_whats_the_brutally_honest_dating_advice_you/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: To all the married women, what is that something your husband does that makes think you won the lottery in marriage aspect of life?\n",
      "ðŸ‘ 5738 | ðŸ’¬ 1217\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n3cp8q/to_all_the_married_women_what_is_that_something/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What screams \"I have a crush on you\"?\n",
      "ðŸ‘ 5713 | ðŸ’¬ 998\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n0yhni/what_screams_i_have_a_crush_on_you/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: People who used the internet before 2001, what did you do in there?\n",
      "ðŸ‘ 5611 | ðŸ’¬ 10828\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n2wosj/people_who_used_the_internet_before_2001_what_did/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What's the most pointless thing youâ€™ve spent hours doing, only to realize it was completely useless?\n",
      "ðŸ‘ 5579 | ðŸ’¬ 2938\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n29otv/whats_the_most_pointless_thing_youve_spent_hours/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What animated TV show is a 10/10?\n",
      "ðŸ‘ 5514 | ðŸ’¬ 7911\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n37wwy/what_animated_tv_show_is_a_1010/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What's the silliest reason you stopped having sex with someone?\n",
      "ðŸ‘ 5015 | ðŸ’¬ 1979\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n4b8b8/whats_the_silliest_reason_you_stopped_having_sex/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: what's a good example of \"half the price, but you use 3Ã— as much\"?\n",
      "ðŸ‘ 4863 | ðŸ’¬ 2090\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n1re8c/whats_a_good_example_of_half_the_price_but_you/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What's an example of NSFW college hazing that you know of?\n",
      "ðŸ‘ 4848 | ðŸ’¬ 1759\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n16wsb/whats_an_example_of_nsfw_college_hazing_that_you/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What are some tricks in bed that everyone should know?\n",
      "ðŸ‘ 4759 | ðŸ’¬ 1745\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n0c76e/what_are_some_tricks_in_bed_that_everyone_should/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: Baby boomers of Reddit, how do you feel about the criticism leveled at your generation for supposedly having destroyed the economy and housing market for the younger generation?\n",
      "ðŸ‘ 4542 | ðŸ’¬ 2776\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n44y5m/baby_boomers_of_reddit_how_do_you_feel_about_the/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: Whatâ€™s the most unhinged, chaotic and downright terrible way to lose weight youâ€™ve ever heard of ?\n",
      "ðŸ‘ 4357 | ðŸ’¬ 3075\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n2ixf1/whats_the_most_unhinged_chaotic_and_downright/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What is one thing that the younger generation believes that is bullshit?\n",
      "ðŸ‘ 4103 | ðŸ’¬ 3811\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n1hewq/what_is_one_thing_that_the_younger_generation/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What screams â€œIâ€™m a horrible spouseâ€?\n",
      "ðŸ‘ 3946 | ðŸ’¬ 1496\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n0qaea/what_screams_im_a_horrible_spouse/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What job requires a PhD and pays poorly but most people donâ€™t realize it?\n",
      "ðŸ‘ 3817 | ðŸ’¬ 811\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n4hxe8/what_job_requires_a_phd_and_pays_poorly_but_most/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What is an experience that you had which you only see in porn movies?\n",
      "ðŸ‘ 3770 | ðŸ’¬ 1956\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n1cgvx/what_is_an_experience_that_you_had_which_you_only/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What are some real-life examples of \"they became the very thing they swore to destroy\"?\n",
      "ðŸ‘ 3758 | ðŸ’¬ 1331\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1mzxtvh/what_are_some_reallife_examples_of_they_became/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What screams â€œIâ€™m a bad parentâ€?\n",
      "ðŸ‘ 3613 | ðŸ’¬ 4050\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n094vk/what_screams_im_a_bad_parent/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: You have a 5 hour drive, you can only listen to music from one band.  Who do you listen to?\n",
      "ðŸ‘ 3199 | ðŸ’¬ 9904\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n2qin4/you_have_a_5_hour_drive_you_can_only_listen_to/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What is the most infamous thing that has occured on Reddit that made the world news?\n",
      "ðŸ‘ 3171 | ðŸ’¬ 1369\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n0eguz/what_is_the_most_infamous_thing_that_has_occured/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What is a historical inaccuracy thats extremely commonly said that really annoys you?\n",
      "ðŸ‘ 3111 | ðŸ’¬ 2168\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n3gfv6/what_is_a_historical_inaccuracy_thats_extremely/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: Which founder would be horrified with the current activities of the company they started?\n",
      "ðŸ‘ 2862 | ðŸ’¬ 1446\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n0m5bi/which_founder_would_be_horrified_with_the_current/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: Which luxury brand is basically just overpriced junk with good marketing?\n",
      "ðŸ‘ 2838 | ðŸ’¬ 2590\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n07pl4/which_luxury_brand_is_basically_just_overpriced/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: Doctors on Reddit, what is the most obvious lie a patient has ever told you?\n",
      "ðŸ‘ 2749 | ðŸ’¬ 1782\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n040vt/doctors_on_reddit_what_is_the_most_obvious_lie_a/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What are you slowly realizing as you get older ?\n",
      "ðŸ‘ 2595 | ðŸ’¬ 1407\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n2nz7p/what_are_you_slowly_realizing_as_you_get_older/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What are some good \"you have no concept of time\" facts?\n",
      "ðŸ‘ 2576 | ðŸ’¬ 875\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n233kf/what_are_some_good_you_have_no_concept_of_time/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What is the biggest waste of money that people still defend?\n",
      "ðŸ‘ 2560 | ðŸ’¬ 4128\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n0z2t1/what_is_the_biggest_waste_of_money_that_people/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: Whatâ€™s actually safe, but people think is really dangerous?\n",
      "ðŸ‘ 2511 | ðŸ’¬ 4043\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1mzptm1/whats_actually_safe_but_people_think_is_really/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: Naturally thin people, what do you guys eat in a day?\n",
      "ðŸ‘ 2307 | ðŸ’¬ 2867\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n2aok5/naturally_thin_people_what_do_you_guys_eat_in_a/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: Whatâ€™s the most obvious case of a company ruining their own product on purpose so youâ€™d have to keep buying replacements?\n",
      "ðŸ‘ 2275 | ðŸ’¬ 1331\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n20db2/whats_the_most_obvious_case_of_a_company_ruining/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: Have you ever seen a stranger nude by accident? If so, how did it happen?\n",
      "ðŸ‘ 2112 | ðŸ’¬ 899\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n4tvw9/have_you_ever_seen_a_stranger_nude_by_accident_if/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: People who left their home countries due to political unrest, when did you realize it was time to go?\n",
      "ðŸ‘ 2045 | ðŸ’¬ 563\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n43qm3/people_who_left_their_home_countries_due_to/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: Whatâ€™s something that youâ€™re pretty convinced will happen before the end of 2025?\n",
      "ðŸ‘ 1941 | ðŸ’¬ 1739\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1mzld8m/whats_something_that_youre_pretty_convinced_will/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: Whatâ€™s a warning sign society is ignoring right now?\n",
      "ðŸ‘ 1892 | ðŸ’¬ 1684\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n0kgny/whats_a_warning_sign_society_is_ignoring_right_now/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What's your nsfw secret you would take to your grave?\n",
      "ðŸ‘ 1880 | ðŸ’¬ 2095\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n0dieh/whats_your_nsfw_secret_you_would_take_to_your/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What feels amazing that's NOT sexual?\n",
      "ðŸ‘ 1817 | ðŸ’¬ 3042\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n1z4xn/what_feels_amazing_thats_not_sexual/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What industry would collapse overnight if people suddenly became smarter with their money?\n",
      "ðŸ‘ 1811 | ðŸ’¬ 1234\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n041s5/what_industry_would_collapse_overnight_if_people/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: People who grew up poor but are now financially stable, what is a \"poverty habit\" you can't seem to shake?\n",
      "ðŸ‘ 1800 | ðŸ’¬ 1379\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n4jun1/people_who_grew_up_poor_but_are_now_financially/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: What is the best yet smallest sign of seduction?\n",
      "ðŸ‘ 1971 | ðŸ’¬ 342\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n58cvt/what_is_the_best_yet_smallest_sign_of_seduction/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: guys of reddit, whatâ€™s an underrated struggle thatâ€™s unique to being a guy?\n",
      "ðŸ‘ 1776 | ðŸ’¬ 1493\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n1spwu/guys_of_reddit_whats_an_underrated_struggle_thats/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "Title: How did you feel when you realized a presidential candidate could tell obvious lie after obvious lie and still be elected?\n",
      "ðŸ‘ 1783 | ðŸ’¬ 848\n",
      "ðŸ”— https://reddit.com/r/AskReddit/comments/1n55hbg/how_did_you_feel_when_you_realized_a_presidential/\n",
      "ðŸ“ Preview:\n",
      "...\n",
      "\n",
      "\n",
      "âœ… Saved long stories to long_stories.json\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "\n",
    "# Connect to Reddit\n",
    "reddit = praw.Reddit(\n",
    "    client_id=CLIENT_ID,\n",
    "    client_secret=CLIENT_SECRET,\n",
    "    user_agent=USER_AGENT\n",
    ")\n",
    "\n",
    "def fetch_long_stories(subreddit_name=\"AskReddit\", limit=20, min_score=5000, min_length=500):\n",
    "    \"\"\"\n",
    "    Pull long, high-engagement posts from a subreddit.\n",
    "    \n",
    "    Args:\n",
    "        subreddit_name: which subreddit to pull from\n",
    "        limit: how many posts to scan\n",
    "        min_score: minimum upvotes required\n",
    "        min_length: minimum length of the body text (characters)\n",
    "    \"\"\"\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    stories = []\n",
    "\n",
    "    print(f\"\\nðŸ“š Long high-engagement posts from r/{subreddit_name}:\")\n",
    "    for post in subreddit.top(time_filter=\"week\", limit=limit):  # top posts of the week\n",
    "        if post.score >= min_score and len(post.selftext) >= min_length:\n",
    "            story = {\n",
    "                \"title\": post.title,\n",
    "                \"score\": post.score,\n",
    "                \"comments\": post.num_comments,\n",
    "                \"url\": \"https://reddit.com\" + post.permalink,\n",
    "                \"text\": post.selftext\n",
    "            }\n",
    "            stories.append(story)\n",
    "            print(f\"\\nTitle: {story['title']}\")\n",
    "            print(f\"ðŸ‘ {story['score']} | ðŸ’¬ {story['comments']}\")\n",
    "            print(f\"ðŸ”— {story['url']}\")\n",
    "            print(f\"ðŸ“ Preview:\\n{story['text'][:400]}...\\n\")\n",
    "\n",
    "    return stories\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    long_posts = fetch_long_stories(limit=50, min_score=0, min_length=0)\n",
    "\n",
    "    # Save to file\n",
    "    import json\n",
    "    with open(\"long_stories.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(long_posts, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(\"\\nâœ… Saved long stories to long_stories.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7836f3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Mining r/AmItheAsshole ===\n",
      "  [1/10] AITA - Do not want a service dog to participate in my wedding....  (ðŸ‘ 10409 | ðŸ’¬ 9225)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 200\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    194\u001b[39m     \u001b[38;5;66;03m# Example 1: specifically r/AskReddit\u001b[39;00m\n\u001b[32m    195\u001b[39m     \u001b[38;5;66;03m# target = [\"AskReddit\"]\u001b[39;00m\n\u001b[32m    196\u001b[39m \n\u001b[32m    197\u001b[39m     \u001b[38;5;66;03m# Example 2: drama-heavy mix\u001b[39;00m\n\u001b[32m    198\u001b[39m     target = DRAMA_SUBREDDITS + [\u001b[33m\"\u001b[39m\u001b[33mAskReddit\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     best = \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     \u001b[38;5;66;03m# Save\u001b[39;00m\n\u001b[32m    203\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(OUTFILE, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 179\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(subreddits)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(posts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpost.title[:\u001b[32m90\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...  (ðŸ‘ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpost.score\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | ðŸ’¬ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpost.num_comments\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     top_comments = \u001b[43mfetch_best_comments_for_submission\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     results.extend(top_comments)\n\u001b[32m    181\u001b[39m     time.sleep(\u001b[32m1.2\u001b[39m)  \u001b[38;5;66;03m# be gentle to rate limits\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 115\u001b[39m, in \u001b[36mfetch_best_comments_for_submission\u001b[39m\u001b[34m(submission)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# Ensure best/top order\u001b[39;00m\n\u001b[32m    114\u001b[39m submission.comment_sort = \u001b[33m\"\u001b[39m\u001b[33mtop\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m \u001b[43msubmission\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcomments\u001b[49m.replace_more(limit=\u001b[32m0\u001b[39m)  \u001b[38;5;66;03m# expand all \"MoreComments\"\u001b[39;00m\n\u001b[32m    117\u001b[39m collected = []\n\u001b[32m    118\u001b[39m count_scanned = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/content-farm/.venv/lib/python3.13/site-packages/praw/models/reddit/base.py:38\u001b[39m, in \u001b[36mRedditBase.__getattr__\u001b[39m\u001b[34m(self, attribute)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return the value of ``attribute``.\"\"\"\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m attribute.startswith(\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fetched:\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attribute)\n\u001b[32m     40\u001b[39m msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattribute\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/content-farm/.venv/lib/python3.13/site-packages/praw/models/reddit/submission.py:726\u001b[39m, in \u001b[36mSubmission._fetch\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    725\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_fetch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m726\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fetch_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    727\u001b[39m     submission_listing, comment_listing = data\n\u001b[32m    728\u001b[39m     comment_listing = Listing(\u001b[38;5;28mself\u001b[39m._reddit, _data=comment_listing[\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/content-farm/.venv/lib/python3.13/site-packages/praw/models/reddit/submission.py:744\u001b[39m, in \u001b[36mSubmission._fetch_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    742\u001b[39m params.update(\u001b[38;5;28mself\u001b[39m._additional_fetch_params.copy())\n\u001b[32m    743\u001b[39m path = API_PATH[name].format(**fields)\n\u001b[32m--> \u001b[39m\u001b[32m744\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reddit\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/content-farm/.venv/lib/python3.13/site-packages/praw/util/deprecate_args.py:46\u001b[39m, in \u001b[36m_deprecate_args.<locals>.wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     39\u001b[39m     arg_string = _generate_arg_string(_old_args[: \u001b[38;5;28mlen\u001b[39m(args)])\n\u001b[32m     40\u001b[39m     warn(\n\u001b[32m     41\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPositional arguments for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m will no longer be\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     42\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m supported in PRAW 8.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mCall this function with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     43\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m     44\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m     45\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_old_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/content-farm/.venv/lib/python3.13/site-packages/praw/reddit.py:963\u001b[39m, in \u001b[36mReddit.request\u001b[39m\u001b[34m(self, data, files, json, method, params, path)\u001b[39m\n\u001b[32m    961\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ClientException(msg)\n\u001b[32m    962\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m963\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_core\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m BadRequest \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[32m    972\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/content-farm/.venv/lib/python3.13/site-packages/prawcore/sessions.py:328\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, path, data, files, json, params, timeout)\u001b[39m\n\u001b[32m    326\u001b[39m     json[\u001b[33m\"\u001b[39m\u001b[33mapi_type\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mjson\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    327\u001b[39m url = urljoin(\u001b[38;5;28mself\u001b[39m._requestor.oauth_url, path)\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_with_retries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/content-farm/.venv/lib/python3.13/site-packages/prawcore/sessions.py:234\u001b[39m, in \u001b[36mSession._request_with_retries\u001b[39m\u001b[34m(self, data, files, json, method, params, timeout, url, retry_strategy_state)\u001b[39m\n\u001b[32m    232\u001b[39m retry_strategy_state.sleep()\n\u001b[32m    233\u001b[39m \u001b[38;5;28mself\u001b[39m._log_request(data, method, params, url)\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m response, saved_exception = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry_strategy_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m do_retry = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m response.status_code == codes[\u001b[33m\"\u001b[39m\u001b[33munauthorized\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/content-farm/.venv/lib/python3.13/site-packages/prawcore/sessions.py:186\u001b[39m, in \u001b[36mSession._make_request\u001b[39m\u001b[34m(self, data, files, json, method, params, retry_strategy_state, timeout, url)\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_make_request\u001b[39m(\n\u001b[32m    175\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    176\u001b[39m     data: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]],\n\u001b[32m   (...)\u001b[39m\u001b[32m    183\u001b[39m     url: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    184\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[Response, \u001b[38;5;28;01mNone\u001b[39;00m] | \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m]:\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m         response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_rate_limiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_requestor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_set_header_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m            \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m            \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m         log.debug(\n\u001b[32m    199\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mResponse: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m bytes) (rst-\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m:rem-\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m:used-\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m ratelimit) at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    200\u001b[39m             response.status_code,\n\u001b[32m   (...)\u001b[39m\u001b[32m    205\u001b[39m             time.time(),\n\u001b[32m    206\u001b[39m         )\n\u001b[32m    207\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/content-farm/.venv/lib/python3.13/site-packages/prawcore/rate_limit.py:47\u001b[39m, in \u001b[36mRateLimiter.call\u001b[39m\u001b[34m(self, request_function, set_header_callback, *args, **kwargs)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28mself\u001b[39m.delay()\n\u001b[32m     46\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m] = set_header_callback()\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m response = \u001b[43mrequest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;28mself\u001b[39m.update(response.headers)\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/content-farm/.venv/lib/python3.13/site-packages/prawcore/requestor.py:68\u001b[39m, in \u001b[36mRequestor.request\u001b[39m\u001b[34m(self, timeout, *args, **kwargs)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Issue the HTTP request capturing any errors that may occur.\"\"\"\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_http\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:  \u001b[38;5;66;03m# noqa: BLE001\u001b[39;00m\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RequestException(exc, args, kwargs) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/content-farm/.venv/lib/python3.13/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/content-farm/.venv/lib/python3.13/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/content-farm/.venv/lib/python3.13/site-packages/requests/adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    641\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/content-farm/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/content-farm/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/content-farm/.venv/lib/python3.13/site-packages/urllib3/connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    568\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:1428\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1426\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1427\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1428\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1429\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1430\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    333\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    294\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1304\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1300\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1301\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1302\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1303\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1305\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1138\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# pull_best_comments.py\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "import praw\n",
    "from prawcore.exceptions import RequestException, ResponseException, ServerError\n",
    "\n",
    "# ----------------------------\n",
    "# SETTINGS\n",
    "# ----------------------------\n",
    "DRAMA_SUBREDDITS = [\n",
    "    \"AmItheAsshole\", \"BestofRedditorUpdates\", \"relationship_advice\",\n",
    "    \"TrueOffMyChest\", \"relationships\", \"JustNoMIL\", \"entitledparents\",\n",
    "    \"OffMyChest\", \"survivinginfidelity\"\n",
    "]\n",
    "# You can also include 'all' in the list, or just target \"AskReddit\" specifically\n",
    "\n",
    "DRAMA_KEYWORDS = [\n",
    "    \"divorce\",\"cheat\",\"affair\",\"wedding\",\"engagement\",\"pregnant\",\"custody\",\n",
    "    \"family\",\"mother-in-law\",\"entitled\",\"betray\",\"gaslight\",\"toxic\",\"lawsuit\",\n",
    "    \"will\",\"inherit\",\"ex\",\"backfire\",\"secret\",\"lied\",\"scammed\",\"abusive\"\n",
    "]\n",
    "DRAMA_RE = re.compile(r\"\\b(\" + \"|\".join(re.escape(k) for k in DRAMA_KEYWORDS) + r\")\\b\", re.I)\n",
    "\n",
    "# Post filters\n",
    "MIN_POST_SCORE = 2000          # baseline quality gate\n",
    "MIN_POST_COMMENTS = 150\n",
    "POSTS_PER_SUBREDDIT = 10       # how many candidate posts to scan per subreddit\n",
    "POST_TIME_FILTER = \"week\"      # 'day' | 'week' | 'month' | 'year' | 'all'\n",
    "\n",
    "# Comment filters\n",
    "MIN_COMMENT_SCORE = 200\n",
    "MIN_COMMENT_LENGTH = 400       # characters\n",
    "MAX_COMMENTS_TO_SCAN = 600     # safety cap per post to limit API work\n",
    "TOP_COMMENTS_PER_POST = 8      # how many to keep per post\n",
    "\n",
    "# Output\n",
    "OUTFILE = \"best_comments.json\"\n",
    "\n",
    "# ----------------------------\n",
    "# REDDIT CLIENT\n",
    "# ----------------------------\n",
    "reddit = praw.Reddit(\n",
    "    client_id=CLIENT_ID,\n",
    "    client_secret=CLIENT_SECRET,\n",
    "    user_agent=USER_AGENT\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# HELPERS\n",
    "# ----------------------------\n",
    "def looks_removed(text: str) -> bool:\n",
    "    t = (text or \"\").strip().lower()\n",
    "    return t in (\"[removed]\", \"[deleted]\")\n",
    "\n",
    "def comment_engagement_score(c) -> float:\n",
    "    \"\"\"\n",
    "    A simple, tunable scoring function.\n",
    "    We reward: upvotes, awards, replies, controversy, length, and drama keywords.\n",
    "    \"\"\"\n",
    "    score = getattr(c, \"score\", 0) or 0\n",
    "    awards = getattr(c, \"total_awards_received\", 0) or 0\n",
    "    controversiality = getattr(c, \"controversiality\", 0) or 0\n",
    "    body = getattr(c, \"body\", \"\") or \"\"\n",
    "    length = len(body)\n",
    "\n",
    "    # Direct replies count (only loaded if tree expanded; still a decent proxy)\n",
    "    try:\n",
    "        direct_replies = len(c.replies)\n",
    "    except Exception:\n",
    "        direct_replies = 0\n",
    "\n",
    "    # Base: votes + awards + replies\n",
    "    s = score + 50 * awards + 4 * direct_replies\n",
    "\n",
    "    # Heated debates get a bump\n",
    "    s += 120 * controversiality\n",
    "\n",
    "    # Reward long-form (cap the boost)\n",
    "    s += min(length // 300, 6) * 25  # +25 per 300 chars up to 6 chunks\n",
    "\n",
    "    # Drama keywords bonus\n",
    "    if DRAMA_RE.search(body):\n",
    "        s += 120\n",
    "\n",
    "    # OP comments are often interesting but not always ideal for â€œcommentaryâ€ videos.\n",
    "    # If you want to favor OPâ€™s clarifications, uncomment:\n",
    "    # if getattr(c, \"is_submitter\", False):\n",
    "    #     s += 40\n",
    "\n",
    "    return float(s)\n",
    "\n",
    "def fetch_candidate_submissions(subreddit_name: str) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Pull high-engagement posts to mine for comments.\n",
    "    We use .top(time_filter=POST_TIME_FILTER) to bias toward proven engagement.\n",
    "    \"\"\"\n",
    "    sub = reddit.subreddit(subreddit_name)\n",
    "    candidates = []\n",
    "    # You can mix sources; here we use top-week. You can also try rising() for â€œbreaking.â€\n",
    "    for post in sub.top(time_filter=POST_TIME_FILTER, limit=POSTS_PER_SUBREDDIT * 2):\n",
    "        if post.score >= MIN_POST_SCORE and post.num_comments >= MIN_POST_COMMENTS and not post.stickied:\n",
    "            candidates.append(post)\n",
    "            if len(candidates) >= POSTS_PER_SUBREDDIT:\n",
    "                break\n",
    "    return candidates\n",
    "\n",
    "def fetch_best_comments_for_submission(submission) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Expand comment tree, filter, score, and return the best comments.\n",
    "    \"\"\"\n",
    "    # Ensure best/top order\n",
    "    submission.comment_sort = \"top\"\n",
    "    submission.comments.replace_more(limit=0)  # expand all \"MoreComments\"\n",
    "\n",
    "    collected = []\n",
    "    count_scanned = 0\n",
    "\n",
    "    # Flattened traversal (ordered by current sort)\n",
    "    for c in submission.comments.list():\n",
    "        if count_scanned >= MAX_COMMENTS_TO_SCAN:\n",
    "            break\n",
    "        count_scanned += 1\n",
    "\n",
    "        # Skip non-Comment objects defensively\n",
    "        if c.__class__.__name__ != \"Comment\":\n",
    "            continue\n",
    "\n",
    "        body = getattr(c, \"body\", \"\") or \"\"\n",
    "        if looks_removed(body):\n",
    "            continue\n",
    "        if len(body) < MIN_COMMENT_LENGTH:\n",
    "            continue\n",
    "        if getattr(c, \"score\", 0) < MIN_COMMENT_SCORE:\n",
    "            continue\n",
    "        if getattr(c, \"stickied\", False):\n",
    "            continue  # skip auto-mod stickies\n",
    "\n",
    "        s = comment_engagement_score(c)\n",
    "\n",
    "        collected.append({\n",
    "            \"submission_id\": submission.id,\n",
    "            \"submission_title\": submission.title,\n",
    "            \"submission_url\": \"https://reddit.com\" + submission.permalink,\n",
    "            \"submission_score\": submission.score,\n",
    "            \"submission_num_comments\": submission.num_comments,\n",
    "\n",
    "            \"comment_id\": c.id,\n",
    "            \"comment_permalink\": \"https://reddit.com\" + c.permalink,\n",
    "            \"author\": str(getattr(c, \"author\", None)) if getattr(c, \"author\", None) else \"[deleted]\",\n",
    "            \"score\": c.score,\n",
    "            \"awards\": getattr(c, \"total_awards_received\", 0) or 0,\n",
    "            \"controversiality\": getattr(c, \"controversiality\", 0) or 0,\n",
    "            \"is_submitter\": getattr(c, \"is_submitter\", False),\n",
    "            \"direct_replies\": len(c.replies) if hasattr(c, \"replies\") else 0,\n",
    "            \"length\": len(body),\n",
    "            \"body\": body,\n",
    "            \"engagement_score\": s\n",
    "        })\n",
    "\n",
    "    # Sort and take top N\n",
    "    collected.sort(key=lambda x: x[\"engagement_score\"], reverse=True)\n",
    "    return collected[:TOP_COMMENTS_PER_POST]\n",
    "\n",
    "def run(subreddits: List[str]) -> List[Dict[str, Any]]:\n",
    "    results = []\n",
    "    for name in subreddits:\n",
    "        print(f\"\\n=== Mining r/{name} ===\")\n",
    "        try:\n",
    "            posts = fetch_candidate_submissions(name)\n",
    "        except (RequestException, ResponseException, ServerError) as e:\n",
    "            print(f\"Skipping r/{name} due to API error: {e}\")\n",
    "            continue\n",
    "\n",
    "        for i, post in enumerate(posts, 1):\n",
    "            print(f\"  [{i}/{len(posts)}] {post.title[:90]}...  (ðŸ‘ {post.score} | ðŸ’¬ {post.num_comments})\")\n",
    "            try:\n",
    "                top_comments = fetch_best_comments_for_submission(post)\n",
    "                results.extend(top_comments)\n",
    "                time.sleep(1.2)  # be gentle to rate limits\n",
    "            except (RequestException, ResponseException, ServerError) as e:\n",
    "                print(f\"    Failed on comments for {post.id}: {e}\")\n",
    "                time.sleep(1.5)\n",
    "\n",
    "        # short pause between subreddits\n",
    "        time.sleep(1.5)\n",
    "\n",
    "    # Global sort to surface absolute bangers\n",
    "    results.sort(key=lambda x: x[\"engagement_score\"], reverse=True)\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example 1: specifically r/AskReddit\n",
    "    # target = [\"AskReddit\"]\n",
    "\n",
    "    # Example 2: drama-heavy mix\n",
    "    target = DRAMA_SUBREDDITS + [\"AskReddit\"]\n",
    "\n",
    "    best = run(target)\n",
    "\n",
    "    # Save\n",
    "    with open(OUTFILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(best, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\nâœ… Saved {len(best)} high-engagement, long comments â†’ {OUTFILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "668e4ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Mining r/AmItheAsshole ===\n",
      "  > Scanning posts from r/AmItheAsshole (top/week) â€¦\n",
      "  > Found 10 long story posts in r/AmItheAsshole\n",
      "  [1/10] AITA - Do not want a service dog to participate in my wedding....  (ðŸ‘ 10413 | ðŸ’¬ 9225)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n2/rstg0c3j7hv2shgc8szmsj2r0000gp/T/ipykernel_48094/2533098364.py:172: UserWarning: The comments for this submission have already been fetched, so the updated comment_sort will not have any effect.\n",
      "  submission.comment_sort = sort\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [2/10] AITA for telling my sister she wasn't always the chosen one?...  (ðŸ‘ 7989 | ðŸ’¬ 1681)\n",
      "  [3/10] AITA if I donâ€™t clarify that I am a type 2 diabetic?...  (ðŸ‘ 8088 | ðŸ’¬ 1024)\n",
      "  [4/10] AITA Tension After My Father Passed Away and I had to Leave 36 Hours Later Because of My I...  (ðŸ‘ 7010 | ðŸ’¬ 1543)\n",
      "  [5/10] AITA for calling my MIL ill-mannered for going through my suitcase and criticizing my biki...  (ðŸ‘ 6644 | ðŸ’¬ 464)\n",
      "  [6/10] AITA for asking my girlfriend to tell me what flavour of smoothie she wanted?...  (ðŸ‘ 5279 | ðŸ’¬ 620)\n",
      "  [7/10] AITA for ruining to my cousins wedding?...  (ðŸ‘ 4972 | ðŸ’¬ 322)\n",
      "  [8/10] AITA For Leaving Class Early After Being Exposed To My Allergen...  (ðŸ‘ 3941 | ðŸ’¬ 384)\n",
      "  [9/10] AITA for telling my best friend I donâ€™t want her boyfriend to come on my holiday vacation....  (ðŸ‘ 3673 | ðŸ’¬ 211)\n",
      "  [10/10] AITA for refusing to pay my friends back for dinner...  (ðŸ‘ 3353 | ðŸ’¬ 311)\n",
      "\n",
      "=== Mining r/BestofRedditorUpdates ===\n",
      "  > Scanning posts from r/BestofRedditorUpdates (top/week) â€¦\n",
      "  > Found 10 long story posts in r/BestofRedditorUpdates\n",
      "  [1/10] Dad sent me [15f] to boarding school at a young age. Now he's upset that I don't like to s...  (ðŸ‘ 10251 | ðŸ’¬ 968)\n",
      "  [2/10] I think they mixed my dna up with someone elseâ€™s...  (ðŸ‘ 9933 | ðŸ’¬ 318)\n",
      "  [3/10] I found out how my roommate treats my cats when Iâ€™m not home...  (ðŸ‘ 9443 | ðŸ’¬ 304)\n",
      "  [4/10] My wifeâ€™s traumatic childhood is killing my marriage...  (ðŸ‘ 7351 | ðŸ’¬ 822)\n",
      "  [5/10] Is It Possible My Birth Was Never Registered??...  (ðŸ‘ 6228 | ðŸ’¬ 1019)\n",
      "  [6/10] Is it illegal to pretend to offer someone a job in the hopes to make them unemployed?...  (ðŸ‘ 7196 | ðŸ’¬ 332)\n",
      "  [7/10] I hate my ex-husband and his wife. + 2 years update...  (ðŸ‘ 7250 | ðŸ’¬ 260)\n",
      "  [8/10] My mother (52f) cheated and left me (27m) and our family to work in the adult industry and...  (ðŸ‘ 6294 | ðŸ’¬ 522)\n",
      "  [9/10] My coworker acts like my manager and I'm losing my mind....  (ðŸ‘ 6576 | ðŸ’¬ 346)\n",
      "  [10/10] Life guard won't let me back into the beach because my daughter didn't have a top...  (ðŸ‘ 6367 | ðŸ’¬ 329)\n",
      "\n",
      "=== Mining r/relationship_advice ===\n",
      "  > Scanning posts from r/relationship_advice (top/week) â€¦\n",
      "  > Found 5 long story posts in r/relationship_advice\n",
      "  [1/5] I 24m just thought Iâ€™d found my wife 24f dead and I donâ€™t know how to approach this conver...  (ðŸ‘ 4704 | ðŸ’¬ 492)\n",
      "  [2/5] My boyfriend (22M) ate all the jalapeÃ±o poppers I made before I (21F) even got one....  (ðŸ‘ 4255 | ðŸ’¬ 785)\n",
      "  [3/5] My (38F) husband (44M) left to take care of his sick ex-wife (43F). Do I ask for divorce?...  (ðŸ‘ 4295 | ðŸ’¬ 471)\n",
      "  [4/5] FiancÃ©e (33 M)Tracked My (33F) Location to do a â€œPop Upâ€...  (ðŸ‘ 2495 | ðŸ’¬ 565)\n",
      "  [5/5] My [32F] husband [35M] cooked a meal for my friend [29F] and remembered her favorite drink...  (ðŸ‘ 2014 | ðŸ’¬ 748)\n",
      "\n",
      "=== Mining r/TrueOffMyChest ===\n",
      "  > Scanning posts from r/TrueOffMyChest (top/week) â€¦\n",
      "  > Found 8 long story posts in r/TrueOffMyChest\n",
      "  [1/8] Our newborn (1mo) is making me see my wife as \"stupid,\" and it's turning me into an asshol...  (ðŸ‘ 7639 | ðŸ’¬ 1804)\n",
      "  [2/8] My Coworkers wife thought we were having an affair because I made him and their kids dinne...  (ðŸ‘ 5840 | ðŸ’¬ 998)\n",
      "  [3/8] My dad made my mom have sex with him in exchange for taking our family on vacation....  (ðŸ‘ 4728 | ðŸ’¬ 399)\n",
      "  [4/8] I hate my kidâ€™s friend...  (ðŸ‘ 4190 | ðŸ’¬ 382)\n",
      "  [5/8] My hobby was ruined for me and i miss it so much...  (ðŸ‘ 3697 | ðŸ’¬ 415)\n",
      "  [6/8] My sister is famous and nobody knows what she's really like...  (ðŸ‘ 3800 | ðŸ’¬ 159)\n",
      "  [7/8] Got my gf pregnant at 21 and Iâ€™m not impressed....  (ðŸ‘ 2630 | ðŸ’¬ 504)\n",
      "  [8/8] Wife used my past and secrets against me (UPDATE)...  (ðŸ‘ 2407 | ðŸ’¬ 608)\n",
      "\n",
      "=== Mining r/relationships ===\n",
      "  > Scanning posts from r/relationships (top/week) â€¦\n",
      "  > Found 0 long story posts in r/relationships\n",
      "\n",
      "=== Mining r/JustNoMIL ===\n",
      "  > Scanning posts from r/JustNoMIL (top/week) â€¦\n",
      "  > Found 0 long story posts in r/JustNoMIL\n",
      "\n",
      "=== Mining r/entitledparents ===\n",
      "  > Scanning posts from r/entitledparents (top/week) â€¦\n",
      "  > Found 0 long story posts in r/entitledparents\n",
      "\n",
      "=== Mining r/OffMyChest ===\n",
      "  > Scanning posts from r/OffMyChest (top/week) â€¦\n",
      "  > Found 4 long story posts in r/OffMyChest\n",
      "  [1/4] Iâ€™m pregnant. My mom wants to come stay with us after we deliver the baby. Not so she can ...  (ðŸ‘ 6156 | ðŸ’¬ 242)\n",
      "  [2/4] I pretended to be mute for three months and now I canâ€™t stop...  (ðŸ‘ 3650 | ðŸ’¬ 168)\n",
      "  [3/4] My husband is no longer attracted to me after I gave birth but it is worse than I thought....  (ðŸ‘ 3099 | ðŸ’¬ 353)\n",
      "  [4/4] I want to divorce my wife because I resent her for not allowing me to help my sister when ...  (ðŸ‘ 2022 | ðŸ’¬ 710)\n",
      "\n",
      "=== Mining r/survivinginfidelity ===\n",
      "  > Scanning posts from r/survivinginfidelity (top/week) â€¦\n",
      "  > Found 0 long story posts in r/survivinginfidelity\n",
      "\n",
      "=== Mining r/AskReddit ===\n",
      "  > Scanning posts from r/AskReddit (top/week) â€¦\n",
      "  > Found 0 long story posts in r/AskReddit\n",
      "\n",
      "âœ… Saved 37 long story posts â†’ best_posts.json\n",
      "âœ… Saved 37 combined records (post + comments) â†’ stories_with_comments.json\n",
      "\n",
      "âœ… Saved 152 high-engagement, long comments â†’ best_comments.json\n"
     ]
    }
   ],
   "source": [
    "# pull_best_comments.py\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "import praw\n",
    "from prawcore.exceptions import RequestException, ResponseException, ServerError, Forbidden\n",
    "\n",
    "# ----------------------------\n",
    "# SETTINGS\n",
    "# ----------------------------\n",
    "DRAMA_SUBREDDITS = [\n",
    "    \"AmItheAsshole\", \"BestofRedditorUpdates\", \"relationship_advice\",\n",
    "    \"TrueOffMyChest\", \"relationships\", \"JustNoMIL\", \"entitledparents\",\n",
    "    \"OffMyChest\", \"survivinginfidelity\"\n",
    "]\n",
    "# You can also include 'all' in the list, or just target \"AskReddit\" specifically\n",
    "\n",
    "DRAMA_KEYWORDS = [\n",
    "    \"divorce\",\"cheat\",\"affair\",\"wedding\",\"engagement\",\"pregnant\",\"custody\",\n",
    "    \"family\",\"mother-in-law\",\"entitled\",\"betray\",\"gaslight\",\"toxic\",\"lawsuit\",\n",
    "    \"will\",\"inherit\",\"ex\",\"backfire\",\"secret\",\"lied\",\"scammed\",\"abusive\",\"update\",\"boundary\"\n",
    "]\n",
    "DRAMA_RE = re.compile(r\"\\b(\" + \"|\".join(re.escape(k) for k in DRAMA_KEYWORDS) + r\")\\b\", re.I)\n",
    "\n",
    "# Post filters (for **posts** weâ€™ll also use these gates)\n",
    "MIN_POST_SCORE = 2000          # baseline quality gate\n",
    "MIN_POST_COMMENTS = 150\n",
    "POSTS_PER_SUBREDDIT = 10       # how many candidate posts to scan per subreddit\n",
    "POST_TIME_FILTER = \"week\"      # 'day' | 'week' | 'month' | 'year' | 'all'\n",
    "\n",
    "# Extra post gate for long-form stories\n",
    "MIN_POST_LENGTH = 700          # chars in selftext (longer = more likely a full story)\n",
    "\n",
    "# Comment filters\n",
    "MIN_COMMENT_SCORE = 200\n",
    "MIN_COMMENT_LENGTH = 400       # characters\n",
    "MAX_COMMENTS_TO_SCAN = 600     # safety cap per post to limit API work\n",
    "TOP_COMMENTS_PER_POST = 8      # how many to keep per post\n",
    "\n",
    "# Output\n",
    "OUTFILE = \"best_comments.json\"             # (same name as your original)\n",
    "POSTS_OUTFILE = \"best_posts.json\"          # new: long story posts (title + selftext)\n",
    "COMBINED_OUTFILE = \"stories_with_comments.json\"  # new: post + its top comments\n",
    "\n",
    "# ----------------------------\n",
    "# REDDIT CLIENT\n",
    "# (uses your existing globals: CLIENT_ID / CLIENT_SECRET / USER_AGENT)\n",
    "# ----------------------------\n",
    "reddit = praw.Reddit(\n",
    "    client_id=CLIENT_ID,\n",
    "    client_secret=CLIENT_SECRET,\n",
    "    user_agent=USER_AGENT\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# HELPERS\n",
    "# ----------------------------\n",
    "def looks_removed(text: str) -> bool:\n",
    "    t = (text or \"\").strip().lower()\n",
    "    return t in (\"[removed]\", \"[deleted]\")\n",
    "\n",
    "def comment_engagement_score(c) -> float:\n",
    "    \"\"\"\n",
    "    A simple, tunable scoring function.\n",
    "    We reward: upvotes, awards, replies, controversy, length, and drama keywords.\n",
    "    \"\"\"\n",
    "    score = getattr(c, \"score\", 0) or 0\n",
    "    awards = getattr(c, \"total_awards_received\", 0) or 0\n",
    "    controversiality = getattr(c, \"controversiality\", 0) or 0\n",
    "    body = getattr(c, \"body\", \"\") or \"\"\n",
    "    length = len(body)\n",
    "\n",
    "    # Direct replies count (only loaded if tree expanded; still a decent proxy)\n",
    "    try:\n",
    "        direct_replies = len(c.replies)\n",
    "    except Exception:\n",
    "        direct_replies = 0\n",
    "\n",
    "    # Base: votes + awards + replies\n",
    "    s = score + 50 * awards + 4 * direct_replies\n",
    "\n",
    "    # Heated debates get a bump\n",
    "    s += 120 * controversiality\n",
    "\n",
    "    # Reward long-form (cap the boost)\n",
    "    s += min(length // 300, 6) * 25  # +25 per 300 chars up to 6 chunks\n",
    "\n",
    "    # Drama keywords bonus\n",
    "    if DRAMA_RE.search(body):\n",
    "        s += 120\n",
    "\n",
    "    # OP comments are often interesting but not always ideal for â€œcommentaryâ€ videos.\n",
    "    # If you want to favor OPâ€™s clarifications, uncomment:\n",
    "    # if getattr(c, \"is_submitter\", False):\n",
    "    #     s += 40\n",
    "\n",
    "    return float(s)\n",
    "\n",
    "# ----------------------------\n",
    "# POSTS: discovery (long story posts)\n",
    "# ----------------------------\n",
    "def fetch_long_story_posts(subreddit_name: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Discover long, engaged **posts** (selftext stories) from a subreddit.\n",
    "    Uses .top() for the configured POST_TIME_FILTER and applies your post gates.\n",
    "    \"\"\"\n",
    "    sub = reddit.subreddit(subreddit_name)\n",
    "    results = []\n",
    "\n",
    "    print(f\"  > Scanning posts from r/{subreddit_name} (top/{POST_TIME_FILTER}) â€¦\")\n",
    "    # Pull a bit wider than POSTS_PER_SUBREDDIT so filters can cut down to ~target\n",
    "    limit = max(POSTS_PER_SUBREDDIT * 5, 30)\n",
    "    for post in sub.top(time_filter=POST_TIME_FILTER, limit=limit):\n",
    "        try:\n",
    "            if post.stickied:\n",
    "                continue\n",
    "            if not getattr(post, \"is_self\", False):\n",
    "                continue\n",
    "\n",
    "            text = (post.selftext or \"\").strip()\n",
    "            if not text or looks_removed(text):\n",
    "                continue\n",
    "            if len(text) < MIN_POST_LENGTH:\n",
    "                continue\n",
    "            if post.score < MIN_POST_SCORE or post.num_comments < MIN_POST_COMMENTS:\n",
    "                continue\n",
    "\n",
    "            row = {\n",
    "                \"id\": post.id,\n",
    "                \"subreddit\": str(post.subreddit),\n",
    "                \"title\": post.title,\n",
    "                \"url\": \"https://reddit.com\" + post.permalink,\n",
    "                \"score\": int(post.score),\n",
    "                \"num_comments\": int(post.num_comments),\n",
    "                \"upvote_ratio\": getattr(post, \"upvote_ratio\", None),\n",
    "                \"created_utc\": float(post.created_utc),\n",
    "                \"length\": len(text),\n",
    "                \"selftext\": text\n",
    "            }\n",
    "            results.append(row)\n",
    "\n",
    "            if len(results) >= POSTS_PER_SUBREDDIT:\n",
    "                break\n",
    "        except Exception:\n",
    "            # Skip weird/unexpected objects\n",
    "            continue\n",
    "\n",
    "    # Rank posts very simply: score + 2*comments (+ keyword bump)\n",
    "    def post_rank_key(p):\n",
    "        base = p[\"score\"] + 2 * p[\"num_comments\"]\n",
    "        if DRAMA_RE.search(p[\"title\"]) or DRAMA_RE.search(p[\"selftext\"]):\n",
    "            base += 200\n",
    "        return base\n",
    "\n",
    "    results.sort(key=post_rank_key, reverse=True)\n",
    "    print(f\"  > Found {len(results)} long story posts in r/{subreddit_name}\")\n",
    "    return results\n",
    "\n",
    "# ----------------------------\n",
    "# COMMENTS: mining (top + controversial)\n",
    "# ----------------------------\n",
    "def fetch_best_comments_for_submission(submission) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Expand comment tree, filter, score, and return the best comments.\n",
    "    We scan **both** 'top' and 'controversial' sorts, with long pauses.\n",
    "    \"\"\"\n",
    "    collected = []\n",
    "    seen_ids = set()\n",
    "    for sort in (\"top\", \"controversial\"):\n",
    "        try:\n",
    "            submission.comment_sort = sort\n",
    "            submission.comments.replace_more(limit=0)  # expand all \"MoreComments\"\n",
    "        except (RequestException, ResponseException, ServerError, Forbidden):\n",
    "            continue\n",
    "\n",
    "        count_scanned = 0\n",
    "        for c in submission.comments.list():\n",
    "            if count_scanned >= MAX_COMMENTS_TO_SCAN:\n",
    "                break\n",
    "            count_scanned += 1\n",
    "\n",
    "            if c.__class__.__name__ != \"Comment\":\n",
    "                continue\n",
    "            if c.id in seen_ids:\n",
    "                continue\n",
    "\n",
    "            body = getattr(c, \"body\", \"\") or \"\"\n",
    "            if looks_removed(body):\n",
    "                continue\n",
    "            if len(body) < MIN_COMMENT_LENGTH:\n",
    "                continue\n",
    "            if getattr(c, \"score\", 0) < MIN_COMMENT_SCORE:\n",
    "                continue\n",
    "            if getattr(c, \"stickied\", False):\n",
    "                continue  # skip auto-mod stickies\n",
    "\n",
    "            s = comment_engagement_score(c)\n",
    "            seen_ids.add(c.id)\n",
    "            collected.append({\n",
    "                \"submission_id\": submission.id,\n",
    "                \"submission_title\": submission.title,\n",
    "                \"submission_url\": \"https://reddit.com\" + submission.permalink,\n",
    "                \"submission_score\": submission.score,\n",
    "                \"submission_num_comments\": submission.num_comments,\n",
    "\n",
    "                \"comment_id\": c.id,\n",
    "                \"comment_permalink\": \"https://reddit.com\" + c.permalink,\n",
    "                \"author\": str(getattr(c, \"author\", None)) if getattr(c, \"author\", None) else \"[deleted]\",\n",
    "                \"score\": int(getattr(c, \"score\", 0) or 0),\n",
    "                \"awards\": int(getattr(c, \"total_awards_received\", 0) or 0),\n",
    "                \"controversiality\": int(getattr(c, \"controversiality\", 0) or 0),\n",
    "                \"is_submitter\": bool(getattr(c, \"is_submitter\", False)),\n",
    "                \"direct_replies\": int(len(c.replies) if hasattr(c, \"replies\") else 0),\n",
    "                \"length\": len(body),\n",
    "                \"body\": body,\n",
    "                \"engagement_score\": s,\n",
    "                \"sort_source\": sort\n",
    "            })\n",
    "\n",
    "        # ~3s rest between sorts\n",
    "        time.sleep(3.0)\n",
    "\n",
    "    # Sort and take top N\n",
    "    collected.sort(key=lambda x: x[\"engagement_score\"], reverse=True)\n",
    "    return collected[:TOP_COMMENTS_PER_POST]\n",
    "\n",
    "# ----------------------------\n",
    "# Orchestrator (keeps same signature)\n",
    "# ----------------------------\n",
    "def run(subreddits: List[str]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    EXACT same call shape as your original: returns a flat list of top comments\n",
    "    and (side effect) also writes posts and combined files.\n",
    "    \"\"\"\n",
    "    all_posts = []\n",
    "    all_comments = []\n",
    "    combined = []\n",
    "\n",
    "    for name in subreddits:\n",
    "        print(f\"\\n=== Mining r/{name} ===\")\n",
    "\n",
    "        # 1) Pull **posts** (long-form stories)\n",
    "        try:\n",
    "            posts = fetch_long_story_posts(name)\n",
    "            all_posts.extend(posts)\n",
    "        except (RequestException, ResponseException, ServerError, Forbidden) as e:\n",
    "            print(f\"Skipping r/{name} due to API error (posts): {e}\")\n",
    "            # 3s rest even on error\n",
    "            time.sleep(3.0)\n",
    "            continue\n",
    "\n",
    "        # 2) For each post, pull **comments**\n",
    "        for i, post in enumerate(posts, 1):\n",
    "            print(f\"  [{i}/{len(posts)}] {post['title'][:90]}...  (ðŸ‘ {post['score']} | ðŸ’¬ {post['num_comments']})\")\n",
    "            try:\n",
    "                subm = reddit.submission(id=post[\"id\"])\n",
    "                top_comments = fetch_best_comments_for_submission(subm)\n",
    "                all_comments.extend(top_comments)\n",
    "                combined.append({\n",
    "                    \"post\": post,\n",
    "                    \"top_comments\": top_comments\n",
    "                })\n",
    "            except (RequestException, ResponseException, ServerError, Forbidden) as e:\n",
    "                print(f\"    Failed on comments for {post['id']}: {e}\")\n",
    "\n",
    "            # ~3s rest between submissions\n",
    "            time.sleep(3.0)\n",
    "\n",
    "        # ~3s rest between subreddits\n",
    "        time.sleep(3.0)\n",
    "\n",
    "    # Sort outputs\n",
    "    all_comments.sort(key=lambda x: x[\"engagement_score\"], reverse=True)\n",
    "\n",
    "    # Persist extra files here (side effects)\n",
    "    try:\n",
    "        with open(POSTS_OUTFILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(all_posts, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"\\nâœ… Saved {len(all_posts)} long story posts â†’ {POSTS_OUTFILE}\")\n",
    "\n",
    "        with open(COMBINED_OUTFILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(combined, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"âœ… Saved {len(combined)} combined records (post + comments) â†’ {COMBINED_OUTFILE}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Failed writing posts/combined files: {e}\")\n",
    "\n",
    "    # Return comments (so your main keeps working identically)\n",
    "    return all_comments\n",
    "\n",
    "# ----------------------------\n",
    "# MAIN (unchanged calling style)\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Example 1: specifically r/AskReddit\n",
    "    # target = [\"AskReddit\"]\n",
    "\n",
    "    # Example 2: drama-heavy mix\n",
    "    target = DRAMA_SUBREDDITS + [\"AskReddit\"]\n",
    "\n",
    "    best = run(target)\n",
    "\n",
    "    # Save (same as your original)\n",
    "    with open(OUTFILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(best, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\nâœ… Saved {len(best)} high-engagement, long comments â†’ {OUTFILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22d5cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
